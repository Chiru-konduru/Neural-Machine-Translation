{"cells":[{"cell_type":"markdown","metadata":{"id":"gau9xEXMGY8s"},"source":["# CS 447 Homework 3 $-$ Neural Machine Translation\n","In this homework we are going to perform machine translation using two deep learning approaches: a Recurrent Neural Network (RNN) and Transformer.\n","\n","Specifically, we are going to train sequence to sequence models for Spanish to English translation. In this assignment you only need to implement the neural network models, we implement all the data loading for you. Please **refer** to the following resources for more details:\n","\n","1.   https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf\n","2.   https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n","3. https://arxiv.org/pdf/1409.0473.pdf\n","\n","<font color='green'>While you work, we suggest that you keep your hardware accelerator set to \"CPU\" (the default for Colab). However, when you have finished debugging and are ready to train your models, you should select \"GPU\" as your runtime type. This will speed up the training of your models. You can find this by going to <TT>Runtime > Change Runtime Type</TT> and select \"GPU\" from the dropdown menu.</font>\n","\n","As usual, you should not import any other libraries.\n"]},{"cell_type":"markdown","metadata":{"id":"yvR1U28i6Itb"},"source":["# Step 1: Download & Prepare the Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OF-5Ml1jz1HG"},"outputs":[],"source":["### DO NOT EDIT ###\n","\n","import pandas as pd\n","import unicodedata\n","import re\n","from torch.utils.data import Dataset\n","import torch\n","import random\n","import os\n","rnn_encoder, rnn_encoder, transformer_encoder, transformer_decoder = None, None, None, None"]},{"cell_type":"markdown","metadata":{"id":"KiXPRWasyIj1"},"source":["## Helper Functions\n","This cell contains helper functions for the dataloader."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k_WR8vEGMQyS"},"outputs":[],"source":["### DO NOT EDIT ###\n","\n","# Converts the unicode file to ascii\n","def unicode_to_ascii(s):\n","    \"\"\"Normalizes latin chars with accent to their canonical decomposition\"\"\"\n","    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n","\n","\n","def preprocess_sentence(w):\n","    '''\n","    Preprocess the sentence to add the start, end tokens and make them lower-case\n","    '''\n","    w = unicode_to_ascii(w.lower().strip())\n","    w = re.sub(r'([?.!,¿])', r' \\1 ', w)\n","    w = re.sub(r'[\" \"]+', ' ', w)\n","\n","    w = re.sub(r'[^a-zA-Z?.!,¿]+', ' ', w)\n","    \n","    w = w.rstrip().strip()\n","    w = '<start> ' + w + ' <end>'\n","    return w\n","\n","\n","def pad_sequences(x, max_len):\n","    padded = np.zeros((max_len), dtype=np.int64)\n","    if len(x) > max_len:\n","        padded[:] = x[:max_len]\n","    else:\n","        padded[:len(x)] = x\n","    return padded\n","\n","\n","def preprocess_data_to_tensor(dataframe, src_vocab, trg_vocab):\n","    # Vectorize the input and target languages\n","    src_tensor = [[src_vocab.word2idx[s if s in src_vocab.vocab else '<unk>'] for s in es.split(' ')] for es in dataframe['es'].values.tolist()]\n","    trg_tensor = [[trg_vocab.word2idx[s if s in trg_vocab.vocab else '<unk>'] for s in eng.split(' ')] for eng in dataframe['eng'].values.tolist()]\n","\n","    # Calculate the max_length of input and output tensor for padding\n","    max_length_src, max_length_trg = max(len(t) for t in src_tensor), max(len(t) for t in trg_tensor)\n","    print('max_length_src: {}, max_length_trg: {}'.format(max_length_src, max_length_trg))\n","\n","    # Pad all the sentences in the dataset with the max_length\n","    src_tensor = [pad_sequences(x, max_length_src) for x in src_tensor]\n","    trg_tensor = [pad_sequences(x, max_length_trg) for x in trg_tensor]\n","\n","    return src_tensor, trg_tensor, max_length_src, max_length_trg\n","\n","\n","def train_test_split(src_tensor, trg_tensor):\n","    '''\n","    Create training and test sets.\n","    '''\n","    total_num_examples = len(src_tensor) - int(0.2*len(src_tensor))\n","    src_tensor_train, src_tensor_test = src_tensor[:int(0.75*total_num_examples)], src_tensor[int(0.75*total_num_examples):total_num_examples]\n","    trg_tensor_train, trg_tensor_test = trg_tensor[:int(0.75*total_num_examples)], trg_tensor[int(0.75*total_num_examples):total_num_examples]\n","\n","    return src_tensor_train, src_tensor_test, trg_tensor_train, trg_tensor_test"]},{"cell_type":"markdown","metadata":{"id":"-XH8nu0ojQpt"},"source":["## Download and Visualize the Data\n","\n","Here we will download the translation data. We will learn a model to translate Spanish to English."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KtyBFlMKIg7g"},"outputs":[],"source":["### DO NOT EDIT ###\n","\n","if __name__ == '__main__':\n","    os.system(\"wget http://www.manythings.org/anki/spa-eng.zip\")\n","    os.system(\"unzip -o spa-eng.zip\")"]},{"cell_type":"markdown","metadata":{"id":"q3PqS2IJx0pt"},"source":["Now we visualize the data."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1069,"status":"ok","timestamp":1667761153049,"user":{"displayName":"Chiranjeevi Konduru","userId":"10258871021540197644"},"user_tz":360},"id":"4WNjizED6eFI","outputId":"0142af1d-c9c9-4cbe-c510-81dfaeda11c4"},"outputs":[{"name":"stdout","output_type":"stream","text":["                           eng                              es\n","0           I'm eating a pear.        Estoy comiendo una pera.\n","1        Tom is a Red Sox fan.   Tom es un fan de los Red Sox.\n","2                Tom is harsh.                   Tom es cruel.\n","3           Tom is downstairs.                 Tom está abajo.\n","4        It's raining outside.           Fuera está lloviendo.\n","...                        ...                             ...\n","49995             Show me how.                 Muéstrame cómo.\n","49996   Let's play volleyball.           Juguemos al voleibol.\n","49997                 Show me.                     Enséñamelo.\n","49998  It's too late for them.  Es demasiado tarde para ellos.\n","49999   I'd rather live alone.            Prefiero vivir solo.\n","\n","[50000 rows x 2 columns]\n"]}],"source":["### DO NOT EDIT ###\n","\n","if __name__ == '__main__':\n","    lines = open('spa.txt', encoding='UTF-8').read().strip().split('\\n')\n","    total_num_examples = 50000 \n","    original_word_pairs = [[w for w in l.split('\\t')][:2] for l in lines[:total_num_examples]]\n","    random.shuffle(original_word_pairs)\n","\n","    dat = pd.DataFrame(original_word_pairs, columns=['eng', 'es'])\n","    print(dat) # Visualize the data"]},{"cell_type":"markdown","metadata":{"id":"v-fp16WlI7D_"},"source":["Next we preprocess the data."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2718,"status":"ok","timestamp":1667761155765,"user":{"displayName":"Chiranjeevi Konduru","userId":"10258871021540197644"},"user_tz":360},"id":"Mud7HbQUMUHB","outputId":"f2b0e678-d523-4f8e-f7f4-bd6b1d5b91dc"},"outputs":[{"name":"stdout","output_type":"stream","text":["                                          eng  \\\n","0           <start> i m eating a pear . <end>   \n","1        <start> tom is a red sox fan . <end>   \n","2                <start> tom is harsh . <end>   \n","3           <start> tom is downstairs . <end>   \n","4        <start> it s raining outside . <end>   \n","...                                       ...   \n","49995             <start> show me how . <end>   \n","49996   <start> let s play volleyball . <end>   \n","49997                 <start> show me . <end>   \n","49998  <start> it s too late for them . <end>   \n","49999   <start> i d rather live alone . <end>   \n","\n","                                                  es  \n","0            <start> estoy comiendo una pera . <end>  \n","1       <start> tom es un fan de los red sox . <end>  \n","2                       <start> tom es cruel . <end>  \n","3                     <start> tom esta abajo . <end>  \n","4               <start> fuera esta lloviendo . <end>  \n","...                                              ...  \n","49995                 <start> muestrame como . <end>  \n","49996           <start> juguemos al voleibol . <end>  \n","49997                     <start> ensenamelo . <end>  \n","49998  <start> es demasiado tarde para ellos . <end>  \n","49999            <start> prefiero vivir solo . <end>  \n","\n","[50000 rows x 2 columns]\n"]}],"source":["### DO NOT EDIT ###\n","\n","if __name__ == '__main__':\n","    data = dat.copy()\n","    data['eng'] = dat.eng.apply(lambda w: preprocess_sentence(w))\n","    data['es'] = dat.es.apply(lambda w: preprocess_sentence(w))\n","    print(data) # Visualizing the data"]},{"cell_type":"markdown","metadata":{"id":"IHJw_CyykmMp"},"source":["## Vocabulary & Dataloader Classes\n","\n","First we create a class for managing our vocabulary as we did in Homework 2. In this homework, we have a separate class for the vocabulary as we need 2 different vocabularies $-$ one for English and one for Spanish.\n","\n","Then we prepare the dataloader and make sure it returns the source sentence and target sentence."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1h4Q21azMW-T"},"outputs":[],"source":["### DO NOT EDIT ###\n","\n","class Vocab_Lang():\n","    def __init__(self, vocab):\n","        self.word2idx = {'<pad>': 0, '<unk>': 1}\n","        self.idx2word = {0: '<pad>', 1: '<unk>'}\n","        self.vocab = vocab\n","        \n","        for index, word in enumerate(vocab):\n","            self.word2idx[word] = index + 2 # +2 because of <pad> and <unk> token\n","            self.idx2word[index + 2] = word\n","    \n","    def __len__(self):\n","        return len(self.word2idx)\n","\n","class MyData(Dataset):\n","    def __init__(self, X, y):\n","        self.length = torch.LongTensor([np.sum(1 - np.equal(x, 0)) for x in X])\n","        self.data = torch.LongTensor(X)\n","        self.target = torch.LongTensor(y)\n","    \n","    def __getitem__(self, index):\n","        x = self.data[index]\n","        y = self.target[index]\n","        return x, y\n","\n","    def __len__(self):\n","        return len(self.data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BA4lG_4fqL9U"},"outputs":[],"source":["### DO NOT EDIT ###\n","\n","import numpy as np\n","import random\n","from torch.utils.data import DataLoader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BrgMtGxHoqFi"},"outputs":[],"source":["### DO NOT EDIT ###\n","\n","if __name__ == '__main__':\n","    # HYPERPARAMETERS (You may change these if you want, though you shouldn't need to)\n","    BATCH_SIZE = 64\n","    EMBEDDING_DIM = 256"]},{"cell_type":"markdown","metadata":{"id":"TzUJ0beWWi7A"},"source":["## Build Vocabulary"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WrXOzSbAWrkT"},"outputs":[],"source":["### DO NOT EDIT ###\n","\n","def build_vocabulary(pd_dataframe):\n","    sentences = [sen.split() for sen in pd_dataframe]\n","    vocab = {}\n","    for sen in sentences:\n","        for word in sen:\n","            if word not in vocab:\n","                vocab[word] = 1\n","    return list(vocab.keys())\n","\n","if __name__ == '__main__':\n","    src_vocab_list = build_vocabulary(data['es'])\n","    trg_vocab_list = build_vocabulary(data['eng'])"]},{"cell_type":"markdown","metadata":{"id":"46wvdhg_17bG"},"source":["## Instantiate Datasets\n","\n","We instantiate our training and validation datasets."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8632,"status":"ok","timestamp":1667761164393,"user":{"displayName":"Chiranjeevi Konduru","userId":"10258871021540197644"},"user_tz":360},"id":"k6UsDOA2c-G6","outputId":"8445e0e0-d7af-480d-e397-8c69a5d65e2d"},"outputs":[{"name":"stdout","output_type":"stream","text":["max_length_src: 16, max_length_trg: 12\n"]}],"source":["### DO NOT EDIT ###\n","\n","if __name__ == '__main__':\n","    src_vocab = Vocab_Lang(src_vocab_list)\n","    trg_vocab = Vocab_Lang(trg_vocab_list)\n","\n","    src_tensor, trg_tensor, max_length_src, max_length_trg = preprocess_data_to_tensor(data, src_vocab, trg_vocab)\n","    src_tensor_train, src_tensor_val, trg_tensor_train, trg_tensor_val = train_test_split(src_tensor, trg_tensor)\n","\n","    # Create train and val datasets\n","    train_dataset = MyData(src_tensor_train, trg_tensor_train)\n","    train_dataset = DataLoader(train_dataset, batch_size=BATCH_SIZE, drop_last=True, shuffle=True)\n","\n","    test_dataset = MyData(src_tensor_val, trg_tensor_val)\n","    test_dataset = DataLoader(test_dataset, batch_size=BATCH_SIZE, drop_last=True, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":42,"status":"ok","timestamp":1667761164394,"user":{"displayName":"Chiranjeevi Konduru","userId":"10258871021540197644"},"user_tz":360},"id":"zWO5ptloc-HL","outputId":"882da1c0-c205-4937-8305-1a72d1655959"},"outputs":[{"name":"stdout","output_type":"stream","text":["Source: tensor([[   2, 2963,  168,  169,   14,  379,    7,    8,    0,    0,    0,    0,\n","            0,    0,    0,    0],\n","        [   2,  398,  807,   23, 4412,  178,  354,    7,    8,    0,    0,    0,\n","            0,    0,    0,    0],\n","        [   2,  454, 5829,    7,    8,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0],\n","        [   2,  248,   39,  842,    7,    8,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0],\n","        [   2, 1315, 1387,  196,   37,    7,    8,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0]])\n","Source Dimensions:  torch.Size([5, 16])\n","Target: tensor([[   2,    3,  775,  160,  326,  321,    8,    9,    0,    0,    0,    0],\n","        [   2,    3,  155,  582, 1134,  267,   27,    8,    9,    0,    0,    0],\n","        [   2,    3,   55,  428,    8,    9,    0,    0,    0,    0,    0,    0],\n","        [   2,    3,  278,   22,  687,    8,    9,    0,    0,    0,    0,    0],\n","        [   2,    3, 1120,   83,   94, 1117,    8,    9,    0,    0,    0,    0]])\n","Target Dimensions:  torch.Size([5, 12])\n"]}],"source":["### DO NOT EDIT ###\n","\n","if __name__ == '__main__':\n","    idxes = random.choices(range(len(train_dataset.dataset)), k=5)\n","    src, trg =  train_dataset.dataset[idxes]\n","    print('Source:', src)\n","    print('Source Dimensions: ', src.size())\n","    print('Target:', trg)\n","    print('Target Dimensions: ', trg.size())"]},{"cell_type":"markdown","metadata":{"id":"amI0Dl7p64TI"},"source":["# Step 2: Train a Recurrent Neural Network (RNN) [45 points]\n","\n","Here you will write a recurrent model for machine translation, and then train and evaluate its results.\n","\n","Here are some links that you may find helpful:\n","1. Attention paper: https://arxiv.org/pdf/1409.0473.pdf\n","2. Explanation of LSTM's & GRU's: https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21\n","3. Attention explanation: https://towardsdatascience.com/attention-in-neural-networks-e66920838742 \n","4. Another attention explanation: https://towardsdatascience.com/attention-and-its-different-forms-7fc3674d14dc\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KSHdxD658EzI"},"outputs":[],"source":["### DO NOT EDIT ###\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import time\n","from tqdm.notebook import tqdm\n","import nltk\n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction, corpus_bleu"]},{"cell_type":"markdown","metadata":{"id":"ENeT1fj_2f8t"},"source":["## <font color='red'>TODO:</font> Encoder Model [10 points]\n","\n","First we build a recurrent encoder model, which will be very similar to what you did in Homework 2. However, instead of using a fully connected layer as the output, you should the return a sequence of outputs of your GRU as well as the final hidden state. These will be used in the decoder.\n","\n","In this cell, you should implement the `__init(...)` and `forward(...)` functions, each of which is <b>5 points</b>."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Sx4QQd3M4XK"},"outputs":[],"source":["from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","\n","class RnnEncoder(nn.Module):\n","    def __init__(self, src_vocab, embedding_dim, hidden_units):\n","        super(RnnEncoder, self).__init__()\n","        \"\"\"\n","        Args:\n","            src_vocab: Vocab_Lang, the source vocabulary\n","            embedding_dim: the dimension of the embedding\n","            hidden_units: The number of features in the GRU hidden state\n","        \"\"\"\n","        self.src_vocab = src_vocab # Do not change\n","        vocab_size = len(src_vocab)\n","\n","        ### TODO ###\n","\n","        # Initialize embedding layer\n","        self.hidden_units = hidden_units\n","        self.embedding_dim = embedding_dim\n","        self.embedding = nn.Embedding(vocab_size, self.embedding_dim)\n","\n","        # Initialize a single directional GRU with 1 layer and batch_first=False\n","        self.gru = nn.GRU( self.embedding_dim, self.hidden_units, batch_first = False)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Args:\n","            x: source texts, [max_len, batch_size]\n","\n","        Returns:\n","            output: [max_len, batch_size, hidden_units]\n","            hidden_state: [1, batch_size, hidden_units] \n","        \n","        Pseudo-code:\n","        - Pass x through an embedding layer and pass the results through the recurrent net\n","        - Return output and hidden states from the recurrent net\n","        \"\"\" \n","        ### TODO ###\n","        embeds = self.embedding(x)\n","        output, hidden_state = self.gru(embeds)\n","        \n","        return output, hidden_state"]},{"cell_type":"markdown","metadata":{"id":"u1L7SPREkFvu"},"source":["## Sanity Check: RNN Encoder Model\n","\n","The code below runs a sanity check for your `RnnEncoder` class. The tests are similar to the hidden ones in Gradescope. However, note that passing the sanity check does <b>not</b> guarantee that you will pass the autograder; it is intended to help you debug."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zQd_cgiblDel"},"outputs":[],"source":["### DO NOT EDIT ###\n","\n","count_parameters = lambda model: sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","def sanityCheckModel(all_test_params, NN, expected_outputs, init_or_forward, data_loader):\n","    print('--- TEST: ' + ('Number of Model Parameters (tests __init__(...))' if init_or_forward=='init' else 'Output shape of forward(...)') + ' ---')\n","    if init_or_forward == \"forward\":\n","        # Reading the first batch of data for testing\n","        for texts_, labels_ in data_loader:\n","            texts_batch, labels_batch = texts_, labels_\n","            break\n","\n","    for tp_idx, (test_params, expected_output) in enumerate(zip(all_test_params, expected_outputs)):       \n","        if init_or_forward == \"forward\":\n","            batch_size = test_params['batch_size']\n","            texts = texts_batch[:batch_size]\n","            if NN.__name__ == \"RnnEncoder\":\n","                texts = texts.transpose(0,1)\n","\n","        # Construct the student model\n","        tps = {k:v for k, v in test_params.items() if k != 'batch_size'}\n","        stu_nn = NN(**tps)\n","\n","        input_rep = str({k:v for k,v in tps.items()})\n","\n","        if init_or_forward == \"forward\":\n","            with torch.no_grad():\n","                if NN.__name__ == \"TransformerEncoder\":\n","                    stu_out = stu_nn(texts)\n","                else:\n","                    stu_out, _ = stu_nn(texts)\n","                    expected_output = torch.rand(expected_output).transpose(0, 1).size()\n","            ref_out_shape = expected_output\n","\n","            has_passed = torch.is_tensor(stu_out)\n","            if not has_passed: msg = 'Output must be a torch.Tensor; received ' + str(type(stu_out))\n","            else: \n","                has_passed = stu_out.shape == ref_out_shape\n","                msg = 'Your Output Shape: ' + str(stu_out.shape)\n","            \n","\n","            status = 'PASSED' if has_passed else 'FAILED'\n","            message = '\\t' + status + \"\\t Init Input: \" + input_rep + '\\tForward Input Shape: ' + str(texts.shape) + '\\tExpected Output Shape: ' + str(ref_out_shape) + '\\t' + msg\n","            print(message)\n","        else:\n","            stu_num_params = count_parameters(stu_nn)\n","            ref_num_params = expected_output\n","            comparison_result = (stu_num_params == ref_num_params)\n","\n","            status = 'PASSED' if comparison_result else 'FAILED'\n","            message = '\\t' + status + \"\\tInput: \" + input_rep + ('\\tExpected Num. Params: ' + str(ref_num_params) + '\\tYour Num. Params: '+ str(stu_num_params))\n","            print(message)\n","\n","        del stu_nn"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1179,"status":"ok","timestamp":1667761165537,"user":{"displayName":"Chiranjeevi Konduru","userId":"10258871021540197644"},"user_tz":360},"id":"OZFuambIkWaD","outputId":"77c62f37-e82c-467f-d84b-e96bcf5f986e"},"outputs":[{"name":"stdout","output_type":"stream","text":["--- TEST: Number of Model Parameters (tests __init__(...)) ---\n","\tPASSED\tInput: {'src_vocab': <__main__.Vocab_Lang object at 0x7f0d6ef39610>, 'embedding_dim': 2, 'hidden_units': 50}\tExpected Num. Params: 33770\tYour Num. Params: 33770\n","\tPASSED\tInput: {'src_vocab': <__main__.Vocab_Lang object at 0x7f0d6ef39610>, 'embedding_dim': 2, 'hidden_units': 100}\tExpected Num. Params: 56870\tYour Num. Params: 56870\n","\tPASSED\tInput: {'src_vocab': <__main__.Vocab_Lang object at 0x7f0d6ef39610>, 'embedding_dim': 2, 'hidden_units': 200}\tExpected Num. Params: 148070\tYour Num. Params: 148070\n","\tPASSED\tInput: {'src_vocab': <__main__.Vocab_Lang object at 0x7f0d6ef39610>, 'embedding_dim': 5, 'hidden_units': 50}\tExpected Num. Params: 72725\tYour Num. Params: 72725\n","\tPASSED\tInput: {'src_vocab': <__main__.Vocab_Lang object at 0x7f0d6ef39610>, 'embedding_dim': 5, 'hidden_units': 100}\tExpected Num. Params: 96275\tYour Num. Params: 96275\n","\tPASSED\tInput: {'src_vocab': <__main__.Vocab_Lang object at 0x7f0d6ef39610>, 'embedding_dim': 5, 'hidden_units': 200}\tExpected Num. Params: 188375\tYour Num. Params: 188375\n","\tPASSED\tInput: {'src_vocab': <__main__.Vocab_Lang object at 0x7f0d6ef39610>, 'embedding_dim': 8, 'hidden_units': 50}\tExpected Num. Params: 111680\tYour Num. Params: 111680\n","\tPASSED\tInput: {'src_vocab': <__main__.Vocab_Lang object at 0x7f0d6ef39610>, 'embedding_dim': 8, 'hidden_units': 100}\tExpected Num. Params: 135680\tYour Num. Params: 135680\n","\tPASSED\tInput: {'src_vocab': <__main__.Vocab_Lang object at 0x7f0d6ef39610>, 'embedding_dim': 8, 'hidden_units': 200}\tExpected Num. Params: 228680\tYour Num. Params: 228680\n","\n","--- TEST: Output shape of forward(...) ---\n","\tPASSED\t Init Input: {'embedding_dim': 256, 'src_vocab': <__main__.Vocab_Lang object at 0x7f0d6ef39610>, 'hidden_units': 50}\tForward Input Shape: torch.Size([16, 1])\tExpected Output Shape: torch.Size([16, 1, 50])\tYour Output Shape: torch.Size([16, 1, 50])\n","\tPASSED\t Init Input: {'embedding_dim': 256, 'src_vocab': <__main__.Vocab_Lang object at 0x7f0d6ef39610>, 'hidden_units': 50}\tForward Input Shape: torch.Size([16, 2])\tExpected Output Shape: torch.Size([16, 2, 50])\tYour Output Shape: torch.Size([16, 2, 50])\n","\tPASSED\t Init Input: {'embedding_dim': 256, 'src_vocab': <__main__.Vocab_Lang object at 0x7f0d6ef39610>, 'hidden_units': 100}\tForward Input Shape: torch.Size([16, 1])\tExpected Output Shape: torch.Size([16, 1, 100])\tYour Output Shape: torch.Size([16, 1, 100])\n","\tPASSED\t Init Input: {'embedding_dim': 256, 'src_vocab': <__main__.Vocab_Lang object at 0x7f0d6ef39610>, 'hidden_units': 100}\tForward Input Shape: torch.Size([16, 2])\tExpected Output Shape: torch.Size([16, 2, 100])\tYour Output Shape: torch.Size([16, 2, 100])\n","\tPASSED\t Init Input: {'embedding_dim': 256, 'src_vocab': <__main__.Vocab_Lang object at 0x7f0d6ef39610>, 'hidden_units': 200}\tForward Input Shape: torch.Size([16, 1])\tExpected Output Shape: torch.Size([16, 1, 200])\tYour Output Shape: torch.Size([16, 1, 200])\n","\tPASSED\t Init Input: {'embedding_dim': 256, 'src_vocab': <__main__.Vocab_Lang object at 0x7f0d6ef39610>, 'hidden_units': 200}\tForward Input Shape: torch.Size([16, 2])\tExpected Output Shape: torch.Size([16, 2, 200])\tYour Output Shape: torch.Size([16, 2, 200])\n"]}],"source":["### DO NOT EDIT ###\n","\n","if __name__ == '__main__':\n","    # Set random seed\n","    torch.manual_seed(42)\n","    # Create test inputs\n","    embedding_dim = [2, 5, 8]\n","    hidden_units = [50, 100, 200]\n","    params = []\n","    inputs = []\n","    for i in range(len(embedding_dim)):\n","        for hu in hidden_units:\n","            inp = {}\n","            inp['src_vocab'] = src_vocab\n","            inp['embedding_dim'] = embedding_dim[i]\n","            inp['hidden_units'] = hu\n","            inputs.append(inp)\n","    # Test init\n","    expected_outputs = [33770, 56870, 148070, 72725, 96275, 188375, 111680, 135680, 228680]\n","\n","    sanityCheckModel(inputs, RnnEncoder, expected_outputs, \"init\", None)\n","    print()\n","\n","    # Test forward\n","    inputs = []\n","    batch_sizes = [1, 2]\n","    for hu in hidden_units:\n","        for b in batch_sizes:\n","            inp = {}\n","            inp['embedding_dim'] = EMBEDDING_DIM\n","            inp['src_vocab'] = src_vocab\n","            inp[\"batch_size\"] = b\n","            inp['hidden_units'] = hu\n","            inputs.append(inp)\n","    # create sanity datasets\n","    sanity_dataset = MyData(src_tensor_train, trg_tensor_train)\n","    sanity_loader = torch.utils.data.DataLoader(sanity_dataset, batch_size=50, num_workers=2, drop_last=True, shuffle=True)\n","    expected_outputs = [torch.Size([1, 16, 50]), torch.Size([2, 16, 50]), torch.Size([1, 16, 100]), torch.Size([2, 16, 100]), torch.Size([1, 16, 200]), torch.Size([2, 16, 200])]\n","\n","    sanityCheckModel(inputs, RnnEncoder, expected_outputs, \"forward\", sanity_loader)"]},{"cell_type":"markdown","metadata":{"id":"vKwsEWpK2mcT"},"source":["## <font color='red'>TODO:</font> Decoder Model [15 points]\n","We will implement a Decoder model that uses an attention mechanism, as provided in https://arxiv.org/pdf/1409.0473.pdf. We have broken this up into three functions that you need to implement: `__init__(self, ...)`, `compute_attention(self, dec_hs, enc_output)`, and `forward(self, x, dec_hs, enc_output)`:\n","\n","* <b>`__init__(self, ...)`: [5 points]</b> Instantiate the parameters of your model, and store them in `self` variables.\n","\n","* <b>`compute_attention(self, dec_hs, enc_output)` [5 points]</b>: Compute the <b>context vector</b>, which is a weighted sum of the encoder output states. Suppose the decoder hidden state at time $t$ is $\\mathbf{h}_t$, and the encoder hidden state at time $s$ is $\\mathbf{\\bar h}_s$. The pseudocode is as follows:\n","\n","  1. <b>Attention scores:</b> Compute real-valued scores for the decoder hidden state $\\mathbf{h}_t$ and each encoder hidden state $\\mathbf{\\bar h}_s$: $$\\mathrm{score}(\\mathbf{h}_t, \\mathbf{\\bar h}_s)=\n","      \\mathbf{v}_a^T \\tanh(\\mathbf{W}_1 \\mathbf{h}_t +\\mathbf{W}_2 \\mathbf{\\bar h}_s)\n","$$\n","   Here you should implement the scoring function. A higher score indicates a stronger \"affinity\" between the decoder state and a specific encoder state. \n","   \n","   <font color='green'><b>Hint:</b> the matrices $\\mathbf{W}_1$, $\\mathbf{W}_2$ and the vector $\\mathbf{v_a}$ can all be implemented with `nn.Linear(...)` in Pytorch.</font>\n","\n","   Note that in theory, $\\mathbf{v_a}$ could have a different dimension than $\\mathbf{h}_t$ and $\\mathbf{\\bar h}_s$, but you should use the same hidden size for this vector.\n","\n"," 2. <b>Attention weights:</b> Normalize the attention scores to obtain a valid probability distribution: $$\\alpha_{ts} = \\frac{\\exp \\big (\\mathrm{score}(\\mathbf{h}_t, \\mathbf{\\bar h}_s) \\big)}{\\sum_{s'=1}^S \\exp \\big (\\mathrm{score}(\\mathbf{h}_t, \\mathbf{\\bar h}_{s'}) \\big)}$$ Notice that this is just the softmax function, and can be implemented with `F.softmax(...)` in Pytorch.\n","\n"," 3. <b>Context vector:</b> Compute a context vector $\\mathbf{c}_t$ that is a weighted average of the encoder hidden states, where the weights are given by the attention weights you just computed: $$\\mathbf{c}_t=\\sum_{s=1}^S \\alpha_{ts} \\mathbf{\\bar h}_s$$\n","\n"," You should return this context vector, along with the attention weights.\n","\n","\n","\n","* <b>`forward(self, x, dec_hs, enc_output)`: [5 points]</b> Run a <b>single</b> decoding step, resulting in a distribution over the vocabulary for the next token in the sequence. Pseudocode can be found in the docstrings below.\n","\n","<font color='green'><b>Hint:</b> You should be able to implement all of this <b>without any for loops</b> using the Pytorch library. Also, remember that these operations should operate in parallel for each item in your batch.</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cw84M2LPM-PC"},"outputs":[],"source":["class RnnDecoder(nn.Module):\n","    def __init__(self, trg_vocab, embedding_dim, hidden_units):\n","        super(RnnDecoder, self).__init__()\n","        \"\"\"\n","        Args:\n","            trg_vocab: Vocab_Lang, the target vocabulary\n","            embedding_dim: The dimension of the embedding\n","            hidden_units: The number of features in the GRU hidden state\n","        \"\"\"\n","        self.trg_vocab = trg_vocab # Do not change\n","        vocab_size = len(trg_vocab)\n","\n","        ### TODO ###\n","\n","        # Initialize embedding layer\n","        self.hidden_units = hidden_units\n","        self.embedding_dim = embedding_dim\n","        self.embedding = nn.Embedding(vocab_size, self.embedding_dim)\n","\n","        # Initialize layers to compute attention score \n","        self.W1 = nn.Linear(self.hidden_units, self.hidden_units)\n","\n","        self.W2 = nn.Linear(self.hidden_units, self.hidden_units)\n","      \n","        self.V = nn.Linear(self.hidden_units,1)\n","\n","        # Initialize a single directional GRU with 1 layer and batch_first=True\n","        self.gru = nn.GRU(self.hidden_units + self.embedding_dim, self.hidden_units, batch_first=True)\n","\n","        # Initialize fully connected layer\n","        self.fc = nn.Linear(self.hidden_units, vocab_size)\n","\n","    def compute_attention(self, dec_hs, enc_output):\n","        '''\n","        This function computes the context vector and attention weights.\n","\n","        Args:\n","            dec_hs: Decoder hidden state; [1, batch_size, hidden_units]\n","            enc_output: Encoder outputs; [max_len_src, batch_size, hidden_units]\n","\n","        Returns:\n","            context_vector: Context vector, according to formula; [batch_size, hidden_units]\n","            attention_weights: The attention weights you have calculated; [batch_size, max_len_src, 1]\n","\n","        Pseudo-code:\n","            (1) Compute the attention scores for dec_hs & enc_output\n","                    - Hint: You may need to permute the dimensions of the tensors in order to pass them through linear layers\n","                    - Output size: [batch_size, max_len_src, 1]\n","            (2) Compute attention_weights by taking a softmax over your scores to normalize the distribution (Make sure that after softmax the normalized scores add up to 1)\n","                    - Output size: [batch_size, max_len_src, 1]\n","            (3) Compute context_vector from attention_weights & enc_output\n","                    - Hint: You may find it helpful to use torch.sum & element-wise multiplication (* operator)\n","            (4) Return context_vector & attention_weights\n","        '''\n","        ### TODO ###\n","        decoder_hidden = dec_hs.permute(1,0,2)\n","        encoder_output = enc_output.permute(1, 0, 2)\n","\n","        #context_vector, attention_weights = None, None\n","        attention_weights = torch.softmax(self.V(torch.tanh(self.W1(decoder_hidden)+self.W2(encoder_output))),dim=1)\n","        context_vector = attention_weights * encoder_output\n","        context_vector = torch.sum(context_vector, dim =1)\n","        \n","        return context_vector, attention_weights\n","\n","    def forward(self, x, dec_hs, enc_output):\n","        '''\n","        This function runs the decoder for a **single** time step.\n","\n","        Args:\n","            x: Input token; [batch_size, 1]\n","            dec_hs: Decoder hidden state; [1, batch_size, hidden_units]\n","            enc_output: Encoder outputs; [max_len_src, batch_size, hidden_units]\n","\n","        Returns:\n","            fc_out: (Unnormalized) output distribution [batch_size, vocab_size]\n","            dec_hs: Decoder hidden state; [1, batch_size, hidden_units]\n","            attention_weights: The attention weights you have learned; [batch_size, max_len_src, 1]\n","\n","        Pseudo-code:\n","            (1) Compute the context vector & attention weights by calling self.compute_attention(...) on the appropriate input\n","            (2) Obtain embedding vectors for your input x\n","                    - Output size: [batch_size, 1, embedding_dim]             \n","            (3) Concatenate the context vector & the embedding vectors along the appropriate dimension\n","            (4) Feed this result through your RNN (along with the current hidden state) to get output and new hidden state\n","                    - Output sizes: [batch_size, 1, hidden_units] & [1, batch_size, hidden_units] \n","            (5) Feed the output of your RNN through linear layer to get (unnormalized) output distribution (don't call softmax!)\n","            (6) Return this output, the new decoder hidden state, & the attention weights\n","        '''\n","        fc_out, attention_weights= None, None\n","\n","        ### TODO ###\n","\n","        context_vector, attention_weights = self.compute_attention(dec_hs, enc_output)\n","\n","        embedding = self.embedding(x)\n","        embed_concat = torch.cat((context_vector.unsqueeze(1), embedding),-1)\n","\n","        output, new_hs = self.gru(embed_concat)\n","        fc_out = self.fc(output).squeeze(1)\n","\n","\n","        return fc_out, dec_hs, attention_weights"]},{"cell_type":"markdown","metadata":{"id":"rG72ATfy1Pjf"},"source":["## Sanity Check: RNN Decoder Model\n","\n","The code below runs a sanity check for your `RnnDecoder` class. The tests are similar to the hidden ones in Gradescope. However, note that passing the sanity check does <b>not</b> guarantee that you will pass the autograder; it is intended to help you debug."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tc4r19oiq4YE"},"outputs":[],"source":["### DO NOT EDIT ###\n","\n","def sanityCheckDecoderModelForward(inputs, NN, expected_outputs):\n","    print('--- TEST: Output shape of forward(...) ---\\n')\n","    expected_fc_outs = expected_outputs[0]\n","    expected_dec_hs = expected_outputs[1]\n","    expected_attention_weights = expected_outputs[2]\n","    msg = ''\n","    for i, inp in enumerate(inputs):\n","        input_rep = '{'\n","        for k,v in inp.items():\n","            if torch.is_tensor(v):\n","                input_rep += str(k) + ': ' + 'Tensor with shape ' + str(v.size()) + ', '\n","            else:\n","                input_rep += str(k) + ': ' + str(v) + ', '\n","        input_rep += '}'\n","        dec = RnnDecoder(trg_vocab=inp['trg_vocab'],embedding_dim=inp['embedding_dim'],hidden_units=inp['hidden_units'])\n","        dec_hs = torch.rand(1, inp[\"batch_size\"], inp['hidden_units'])\n","        x = torch.randint(low=0,high=len(inp[\"trg_vocab\"]),size=(inp[\"batch_size\"], 1))\n","        with torch.no_grad(): \n","            dec_out = dec(x=x, dec_hs=dec_hs,enc_output=inp['encoder_outputs'])\n","            if not isinstance(dec_out, tuple):\n","                msg = '\\tFAILED\\tYour RnnDecoder.forward() output must be a tuple; received ' + str(type(dec_out))\n","                print(msg)\n","                continue\n","            elif len(dec_out)!=3:\n","                msg = '\\tFAILED\\tYour RnnDecoder.forward() output must be a tuple of size 3; received tuple of size ' + str(len(dec_out))\n","                print(msg)\n","                continue\n","            stu_fc_out, stu_dec_hs, stu_attention_weights = dec_out\n","        del dec\n","        has_passed = True\n","        msg = \"\"\n","        if not torch.is_tensor(stu_fc_out):\n","            has_passed = False\n","            msg += '\\tFAILED\\tOutput must be a torch.Tensor; received ' + str(type(stu_fc_out)) + \" \"\n","        if not torch.is_tensor(stu_dec_hs):\n","            has_passed = False\n","            msg += '\\tFAILED\\tDecoder Hidden State must be a torch.Tensor; received ' + str(type(stu_dec_hs)) + \" \"\n","        if not torch.is_tensor(stu_attention_weights):\n","            has_passed = False\n","            msg += '\\tFAILED\\tAttention Weights must be a torch.Tensor; received ' + str(type(stu_attention_weights)) + \" \"\n","        \n","        status = 'PASSED' if has_passed else 'FAILED'\n","        if not has_passed:\n","            message = '\\t' + status + \"\\t Init Input: \" + input_rep + '\\tForward Input Shape: ' + str(inp['encoder_outputs'].shape) + '\\tExpected Output Shape: ' + str(expected_fc_outs[i]) + '\\t' + msg\n","            print(message)\n","            continue\n","        \n","        has_passed = stu_fc_out.size() == expected_fc_outs[i]\n","        msg = 'Your Output Shape: ' + str(stu_fc_out.size())\n","        status = 'PASSED' if has_passed else 'FAILED'\n","        message = '\\t' + status + \"\\t Init Input: \" + input_rep + '\\tForward Input Shape: ' + str(inp['encoder_outputs'].shape) + '\\tExpected Output Shape: ' + str(expected_fc_outs[i]) + '\\t' + msg\n","        print(message)\n","\n","        has_passed = stu_dec_hs.size() == expected_dec_hs[i]\n","        msg = 'Your Hidden State Shape: ' + str(stu_dec_hs.size())\n","        status = 'PASSED' if has_passed else 'FAILED'\n","        message = '\\t' + status + \"\\t Init Input: \" + input_rep + '\\tForward Input Shape: ' + str(inp['encoder_outputs'].shape) + '\\tExpected Hidden State Shape: ' + str(expected_dec_hs[i]) + '\\t' + msg\n","        print(message)\n","\n","        has_passed = stu_attention_weights.size() == expected_attention_weights[i]\n","        msg = 'Your Attention Weights Shape: ' + str(stu_attention_weights.size())\n","        status = 'PASSED' if has_passed else 'FAILED'\n","        message = '\\t' + status + \"\\t Init Input: \" + input_rep + '\\tForward Input Shape: ' + str(inp['encoder_outputs'].shape) + '\\tExpected Attention Weights Shape: ' + str(expected_attention_weights[i]) + '\\t' + msg\n","        print(message)\n","\n","        stu_sum = stu_attention_weights.sum(dim=1).squeeze()\n","        if torch.allclose(stu_sum, torch.ones_like(stu_sum), atol=1e-5):\n","            print('\\tPASSED\\t The sum of your attention_weights along dim 1 is 1.')\n","        else:\n","            print('\\tFAILED\\t The sum of your attention_weights along dim 1 is not 1.')\n","        print()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31,"status":"ok","timestamp":1667761165540,"user":{"displayName":"Chiranjeevi Konduru","userId":"10258871021540197644"},"user_tz":360},"id":"oUplFvOw1O_f","outputId":"66d9e6ce-2562-4ab0-da65-726c17c5a3b6"},"outputs":[{"name":"stdout","output_type":"stream","text":["--- TEST: Number of Model Parameters (tests __init__(...)) ---\n","\tPASSED\tInput: {'trg_vocab': <__main__.Vocab_Lang object at 0x7f0d6ef397d0>, 'embedding_dim': 2, 'hidden_units': 50}\tExpected Num. Params: 371028\tYour Num. Params: 371028\n","\tPASSED\tInput: {'trg_vocab': <__main__.Vocab_Lang object at 0x7f0d6ef397d0>, 'embedding_dim': 2, 'hidden_units': 100}\tExpected Num. Params: 762228\tYour Num. Params: 762228\n","\tPASSED\tInput: {'trg_vocab': <__main__.Vocab_Lang object at 0x7f0d6ef397d0>, 'embedding_dim': 2, 'hidden_units': 200}\tExpected Num. Params: 1664628\tYour Num. Params: 1664628\n","\tPASSED\tInput: {'trg_vocab': <__main__.Vocab_Lang object at 0x7f0d6ef397d0>, 'embedding_dim': 5, 'hidden_units': 50}\tExpected Num. Params: 391305\tYour Num. Params: 391305\n","\tPASSED\tInput: {'trg_vocab': <__main__.Vocab_Lang object at 0x7f0d6ef397d0>, 'embedding_dim': 5, 'hidden_units': 100}\tExpected Num. Params: 782955\tYour Num. Params: 782955\n","\tPASSED\tInput: {'trg_vocab': <__main__.Vocab_Lang object at 0x7f0d6ef397d0>, 'embedding_dim': 5, 'hidden_units': 200}\tExpected Num. Params: 1686255\tYour Num. Params: 1686255\n","\tPASSED\tInput: {'trg_vocab': <__main__.Vocab_Lang object at 0x7f0d6ef397d0>, 'embedding_dim': 8, 'hidden_units': 50}\tExpected Num. Params: 411582\tYour Num. Params: 411582\n","\tPASSED\tInput: {'trg_vocab': <__main__.Vocab_Lang object at 0x7f0d6ef397d0>, 'embedding_dim': 8, 'hidden_units': 100}\tExpected Num. Params: 803682\tYour Num. Params: 803682\n","\tPASSED\tInput: {'trg_vocab': <__main__.Vocab_Lang object at 0x7f0d6ef397d0>, 'embedding_dim': 8, 'hidden_units': 200}\tExpected Num. Params: 1707882\tYour Num. Params: 1707882\n","\n","--- TEST: Output shape of forward(...) ---\n","\n","\tPASSED\t Init Input: {embedding_dim: 50, trg_vocab: <__main__.Vocab_Lang object at 0x7f0d6ef397d0>, batch_size: 1, hidden_units: 50, encoder_outputs: Tensor with shape torch.Size([16, 1, 50]), }\tForward Input Shape: torch.Size([16, 1, 50])\tExpected Output Shape: torch.Size([1, 6609])\tYour Output Shape: torch.Size([1, 6609])\n","\tPASSED\t Init Input: {embedding_dim: 50, trg_vocab: <__main__.Vocab_Lang object at 0x7f0d6ef397d0>, batch_size: 1, hidden_units: 50, encoder_outputs: Tensor with shape torch.Size([16, 1, 50]), }\tForward Input Shape: torch.Size([16, 1, 50])\tExpected Hidden State Shape: torch.Size([1, 1, 50])\tYour Hidden State Shape: torch.Size([1, 1, 50])\n","\tPASSED\t Init Input: {embedding_dim: 50, trg_vocab: <__main__.Vocab_Lang object at 0x7f0d6ef397d0>, batch_size: 1, hidden_units: 50, encoder_outputs: Tensor with shape torch.Size([16, 1, 50]), }\tForward Input Shape: torch.Size([16, 1, 50])\tExpected Attention Weights Shape: torch.Size([1, 16, 1])\tYour Attention Weights Shape: torch.Size([1, 16, 1])\n","\tPASSED\t The sum of your attention_weights along dim 1 is 1.\n","\n","\tPASSED\t Init Input: {embedding_dim: 80, trg_vocab: <__main__.Vocab_Lang object at 0x7f0d6ef397d0>, batch_size: 2, hidden_units: 50, encoder_outputs: Tensor with shape torch.Size([16, 2, 50]), }\tForward Input Shape: torch.Size([16, 2, 50])\tExpected Output Shape: torch.Size([2, 6609])\tYour Output Shape: torch.Size([2, 6609])\n","\tPASSED\t Init Input: {embedding_dim: 80, trg_vocab: <__main__.Vocab_Lang object at 0x7f0d6ef397d0>, batch_size: 2, hidden_units: 50, encoder_outputs: Tensor with shape torch.Size([16, 2, 50]), }\tForward Input Shape: torch.Size([16, 2, 50])\tExpected Hidden State Shape: torch.Size([1, 2, 50])\tYour Hidden State Shape: torch.Size([1, 2, 50])\n","\tPASSED\t Init Input: {embedding_dim: 80, trg_vocab: <__main__.Vocab_Lang object at 0x7f0d6ef397d0>, batch_size: 2, hidden_units: 50, encoder_outputs: Tensor with shape torch.Size([16, 2, 50]), }\tForward Input Shape: torch.Size([16, 2, 50])\tExpected Attention Weights Shape: torch.Size([2, 16, 1])\tYour Attention Weights Shape: torch.Size([2, 16, 1])\n","\tPASSED\t The sum of your attention_weights along dim 1 is 1.\n","\n","\tPASSED\t Init Input: {embedding_dim: 100, trg_vocab: <__main__.Vocab_Lang object at 0x7f0d6ef397d0>, batch_size: 4, hidden_units: 50, encoder_outputs: Tensor with shape torch.Size([16, 4, 50]), }\tForward Input Shape: torch.Size([16, 4, 50])\tExpected Output Shape: torch.Size([4, 6609])\tYour Output Shape: torch.Size([4, 6609])\n","\tPASSED\t Init Input: {embedding_dim: 100, trg_vocab: <__main__.Vocab_Lang object at 0x7f0d6ef397d0>, batch_size: 4, hidden_units: 50, encoder_outputs: Tensor with shape torch.Size([16, 4, 50]), }\tForward Input Shape: torch.Size([16, 4, 50])\tExpected Hidden State Shape: torch.Size([1, 4, 50])\tYour Hidden State Shape: torch.Size([1, 4, 50])\n","\tPASSED\t Init Input: {embedding_dim: 100, trg_vocab: <__main__.Vocab_Lang object at 0x7f0d6ef397d0>, batch_size: 4, hidden_units: 50, encoder_outputs: Tensor with shape torch.Size([16, 4, 50]), }\tForward Input Shape: torch.Size([16, 4, 50])\tExpected Attention Weights Shape: torch.Size([4, 16, 1])\tYour Attention Weights Shape: torch.Size([4, 16, 1])\n","\tPASSED\t The sum of your attention_weights along dim 1 is 1.\n","\n","\tPASSED\t Init Input: {embedding_dim: 120, trg_vocab: <__main__.Vocab_Lang object at 0x7f0d6ef397d0>, batch_size: 1, hidden_units: 100, encoder_outputs: Tensor with shape torch.Size([16, 1, 100]), }\tForward Input Shape: torch.Size([16, 1, 100])\tExpected Output Shape: torch.Size([1, 6609])\tYour Output Shape: torch.Size([1, 6609])\n","\tPASSED\t Init Input: {embedding_dim: 120, trg_vocab: <__main__.Vocab_Lang object at 0x7f0d6ef397d0>, batch_size: 1, hidden_units: 100, encoder_outputs: Tensor with shape torch.Size([16, 1, 100]), }\tForward Input Shape: torch.Size([16, 1, 100])\tExpected Hidden State Shape: torch.Size([1, 1, 100])\tYour Hidden State Shape: torch.Size([1, 1, 100])\n","\tPASSED\t Init Input: {embedding_dim: 120, trg_vocab: <__main__.Vocab_Lang object at 0x7f0d6ef397d0>, batch_size: 1, hidden_units: 100, encoder_outputs: Tensor with shape torch.Size([16, 1, 100]), }\tForward Input Shape: torch.Size([16, 1, 100])\tExpected Attention Weights Shape: torch.Size([1, 16, 1])\tYour Attention Weights Shape: torch.Size([1, 16, 1])\n","\tPASSED\t The sum of your attention_weights along dim 1 is 1.\n","\n","\tPASSED\t Init Input: {embedding_dim: 150, trg_vocab: <__main__.Vocab_Lang object at 0x7f0d6ef397d0>, batch_size: 2, hidden_units: 100, encoder_outputs: Tensor with shape torch.Size([16, 2, 100]), }\tForward Input Shape: torch.Size([16, 2, 100])\tExpected Output Shape: torch.Size([2, 6609])\tYour Output Shape: torch.Size([2, 6609])\n","\tPASSED\t Init Input: {embedding_dim: 150, trg_vocab: <__main__.Vocab_Lang object at 0x7f0d6ef397d0>, batch_size: 2, hidden_units: 100, encoder_outputs: Tensor with shape torch.Size([16, 2, 100]), }\tForward Input Shape: torch.Size([16, 2, 100])\tExpected Hidden State Shape: torch.Size([1, 2, 100])\tYour Hidden State Shape: torch.Size([1, 2, 100])\n","\tPASSED\t Init Input: {embedding_dim: 150, trg_vocab: <__main__.Vocab_Lang object at 0x7f0d6ef397d0>, batch_size: 2, hidden_units: 100, encoder_outputs: Tensor with shape torch.Size([16, 2, 100]), }\tForward Input Shape: torch.Size([16, 2, 100])\tExpected Attention Weights Shape: torch.Size([2, 16, 1])\tYour Attention Weights Shape: torch.Size([2, 16, 1])\n","\tPASSED\t The sum of your attention_weights along dim 1 is 1.\n","\n","\tPASSED\t Init Input: {embedding_dim: 200, trg_vocab: <__main__.Vocab_Lang object at 0x7f0d6ef397d0>, batch_size: 4, hidden_units: 100, encoder_outputs: Tensor with shape torch.Size([16, 4, 100]), }\tForward Input Shape: torch.Size([16, 4, 100])\tExpected Output Shape: torch.Size([4, 6609])\tYour Output Shape: torch.Size([4, 6609])\n","\tPASSED\t Init Input: {embedding_dim: 200, trg_vocab: <__main__.Vocab_Lang object at 0x7f0d6ef397d0>, batch_size: 4, hidden_units: 100, encoder_outputs: Tensor with shape torch.Size([16, 4, 100]), }\tForward Input Shape: torch.Size([16, 4, 100])\tExpected Hidden State Shape: torch.Size([1, 4, 100])\tYour Hidden State Shape: torch.Size([1, 4, 100])\n","\tPASSED\t Init Input: {embedding_dim: 200, trg_vocab: <__main__.Vocab_Lang object at 0x7f0d6ef397d0>, batch_size: 4, hidden_units: 100, encoder_outputs: Tensor with shape torch.Size([16, 4, 100]), }\tForward Input Shape: torch.Size([16, 4, 100])\tExpected Attention Weights Shape: torch.Size([4, 16, 1])\tYour Attention Weights Shape: torch.Size([4, 16, 1])\n","\tPASSED\t The sum of your attention_weights along dim 1 is 1.\n","\n","\tPASSED\t Init Input: {embedding_dim: 300, trg_vocab: <__main__.Vocab_Lang object at 0x7f0d6ef397d0>, batch_size: 1, hidden_units: 200, encoder_outputs: Tensor with shape torch.Size([16, 1, 200]), }\tForward Input Shape: torch.Size([16, 1, 200])\tExpected Output Shape: torch.Size([1, 6609])\tYour Output Shape: torch.Size([1, 6609])\n","\tPASSED\t Init Input: {embedding_dim: 300, trg_vocab: <__main__.Vocab_Lang object at 0x7f0d6ef397d0>, batch_size: 1, hidden_units: 200, encoder_outputs: Tensor with shape torch.Size([16, 1, 200]), }\tForward Input Shape: torch.Size([16, 1, 200])\tExpected Hidden State Shape: torch.Size([1, 1, 200])\tYour Hidden State Shape: torch.Size([1, 1, 200])\n","\tPASSED\t Init Input: {embedding_dim: 300, trg_vocab: <__main__.Vocab_Lang object at 0x7f0d6ef397d0>, batch_size: 1, hidden_units: 200, encoder_outputs: Tensor with shape torch.Size([16, 1, 200]), }\tForward Input Shape: torch.Size([16, 1, 200])\tExpected Attention Weights Shape: torch.Size([1, 16, 1])\tYour Attention Weights Shape: torch.Size([1, 16, 1])\n","\tPASSED\t The sum of your attention_weights along dim 1 is 1.\n","\n","\tPASSED\t Init Input: {embedding_dim: 400, trg_vocab: <__main__.Vocab_Lang object at 0x7f0d6ef397d0>, batch_size: 2, hidden_units: 200, encoder_outputs: Tensor with shape torch.Size([16, 2, 200]), }\tForward Input Shape: torch.Size([16, 2, 200])\tExpected Output Shape: torch.Size([2, 6609])\tYour Output Shape: torch.Size([2, 6609])\n","\tPASSED\t Init Input: {embedding_dim: 400, trg_vocab: <__main__.Vocab_Lang object at 0x7f0d6ef397d0>, batch_size: 2, hidden_units: 200, encoder_outputs: Tensor with shape torch.Size([16, 2, 200]), }\tForward Input Shape: torch.Size([16, 2, 200])\tExpected Hidden State Shape: torch.Size([1, 2, 200])\tYour Hidden State Shape: torch.Size([1, 2, 200])\n","\tPASSED\t Init Input: {embedding_dim: 400, trg_vocab: <__main__.Vocab_Lang object at 0x7f0d6ef397d0>, batch_size: 2, hidden_units: 200, encoder_outputs: Tensor with shape torch.Size([16, 2, 200]), }\tForward Input Shape: torch.Size([16, 2, 200])\tExpected Attention Weights Shape: torch.Size([2, 16, 1])\tYour Attention Weights Shape: torch.Size([2, 16, 1])\n","\tPASSED\t The sum of your attention_weights along dim 1 is 1.\n","\n","\tPASSED\t Init Input: {embedding_dim: 500, trg_vocab: <__main__.Vocab_Lang object at 0x7f0d6ef397d0>, batch_size: 4, hidden_units: 200, encoder_outputs: Tensor with shape torch.Size([16, 4, 200]), }\tForward Input Shape: torch.Size([16, 4, 200])\tExpected Output Shape: torch.Size([4, 6609])\tYour Output Shape: torch.Size([4, 6609])\n","\tPASSED\t Init Input: {embedding_dim: 500, trg_vocab: <__main__.Vocab_Lang object at 0x7f0d6ef397d0>, batch_size: 4, hidden_units: 200, encoder_outputs: Tensor with shape torch.Size([16, 4, 200]), }\tForward Input Shape: torch.Size([16, 4, 200])\tExpected Hidden State Shape: torch.Size([1, 4, 200])\tYour Hidden State Shape: torch.Size([1, 4, 200])\n","\tPASSED\t Init Input: {embedding_dim: 500, trg_vocab: <__main__.Vocab_Lang object at 0x7f0d6ef397d0>, batch_size: 4, hidden_units: 200, encoder_outputs: Tensor with shape torch.Size([16, 4, 200]), }\tForward Input Shape: torch.Size([16, 4, 200])\tExpected Attention Weights Shape: torch.Size([4, 16, 1])\tYour Attention Weights Shape: torch.Size([4, 16, 1])\n","\tPASSED\t The sum of your attention_weights along dim 1 is 1.\n","\n"]}],"source":["### DO NOT EDIT ###\n","\n","if __name__ == '__main__':\n","    # Set random seed\n","    torch.manual_seed(42)\n","    # Create test inputs\n","    embedding_dim = [2, 5, 8]\n","    hidden_units = [50, 100, 200]\n","    params = []\n","    inputs = []\n","    for i in range(len(embedding_dim)):\n","        for hu in hidden_units:\n","            inp = {}\n","            inp['trg_vocab'] = trg_vocab\n","            inp['embedding_dim'] = embedding_dim[i]\n","            inp['hidden_units'] = hu\n","            inputs.append(inp)\n","    # Test init\n","    expected_outputs = [371028, 762228, 1664628, 391305, 782955, 1686255, 411582, 803682, 1707882]\n","    sanityCheckModel(inputs, RnnDecoder, expected_outputs, \"init\", None)\n","    print()\n","\n","    # Test forward\n","    inputs = []\n","    batch_sizes = [1, 2, 4]\n","    embedding_dims = iter([50,80,100,120,150,200,300,400,500])\n","    encoder_outputs = iter([torch.rand([1, 16, 50]), torch.rand([2, 16, 50]), torch.rand([4, 16, 50]), torch.rand([1, 16, 100]), torch.rand([2, 16, 100]), torch.rand([4, 16, 100]), torch.rand([1, 16, 200]), torch.rand([2, 16, 200]),torch.rand([4, 16, 200])])\n","    expected_fc_outs = [torch.Size([1, 6609]),torch.Size([2, 6609]),torch.Size([4, 6609]),torch.Size([1, 6609]),torch.Size([2, 6609]),torch.Size([4, 6609]),torch.Size([1, 6609]),torch.Size([2, 6609]),torch.Size([4, 6609])]\n","    expected_dec_hs = [torch.Size([1, 1, 50]), torch.Size([1, 2, 50]), torch.Size([1, 4, 50]), torch.Size([1, 1, 100]), torch.Size([1, 2, 100]), torch.Size([1, 4, 100]), torch.Size([1, 1, 200]), torch.Size([1, 2, 200]), torch.Size([1, 4, 200])]\n","    expected_attention_weights = [torch.Size([1, 16, 1]), torch.Size([2, 16, 1]), torch.Size([4, 16, 1]), torch.Size([1, 16, 1]), torch.Size([2, 16, 1]), torch.Size([4, 16, 1]), torch.Size([1, 16, 1]), torch.Size([2, 16, 1]), torch.Size([4, 16, 1])]\n","    expected_outputs = (expected_fc_outs, expected_dec_hs, expected_attention_weights)\n","    \n","    for hu in hidden_units:\n","        for b in batch_sizes:\n","            inp = {}\n","            edim = next(embedding_dims)\n","            inp['embedding_dim'] = edim\n","            inp['trg_vocab'] = trg_vocab\n","            inp[\"batch_size\"] = b\n","            inp['hidden_units'] = hu\n","            inp['encoder_outputs'] = next(encoder_outputs).transpose(0,1)\n","            inputs.append(inp)\n","    \n","    sanityCheckDecoderModelForward(inputs, RnnDecoder, expected_outputs)\n"]},{"cell_type":"markdown","metadata":{"id":"VQKYT5w3n82V"},"source":["## Train RNN Model\n","\n","We will train the encoder and decoder using cross-entropy loss."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IyPi_PfkFtSr"},"outputs":[],"source":["### DO NOT EDIT ###\n","\n","def loss_function(real, pred):\n","    mask = real.ge(1).float() # Only consider non-zero inputs in the loss\n","    \n","    loss_ = F.cross_entropy(pred, real) * mask \n","    return torch.mean(loss_)\n","\n","def train_rnn_model(encoder, decoder, dataset, optimizer, trg_vocab, device, n_epochs):\n","    batch_size = dataset.batch_size\n","    for epoch in range(n_epochs):\n","        start = time.time()\n","        n_batch = 0\n","        total_loss = 0\n","        \n","        encoder.train()\n","        decoder.train()\n","        \n","        for src, trg in tqdm(dataset):\n","            n_batch += 1\n","            loss = 0\n","            \n","            enc_output, enc_hidden = encoder(src.transpose(0,1).to(device))\n","            dec_hidden = enc_hidden\n","            \n","            # use teacher forcing - feeding the target as the next input (via dec_input)\n","            dec_input = torch.tensor([[trg_vocab.word2idx['<start>']]] * batch_size)\n","        \n","            # run code below for every timestep in the ys batch\n","            for t in range(1, trg.size(1)):\n","                predictions, dec_hidden, _ = decoder(dec_input.to(device), dec_hidden.to(device), enc_output.to(device))\n","                assert len(predictions.shape) == 2 and predictions.shape[0] == dec_input.shape[0] and predictions.shape[1] == len(trg_vocab.word2idx), \"First output of decoder must have shape [batch_size, vocab_size], you returned shape \" + str(predictions.shape)\n","                loss += loss_function(trg[:, t].to(device), predictions.to(device))\n","                dec_input = trg[:, t].unsqueeze(1)\n","        \n","            batch_loss = (loss / int(trg.size(1)))\n","            total_loss += batch_loss\n","            \n","            optimizer.zero_grad()\n","            \n","            batch_loss.backward()\n","\n","            ### update model parameters\n","            optimizer.step()\n","        \n","        ### TODO: Save checkpoint for model (optional)\n","        print('Epoch:{:2d}/{}\\t Loss: {:.4f} \\t({:.2f}s)'.format(epoch + 1, n_epochs, total_loss / n_batch, time.time() - start))\n","\n","    print('Model trained!')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1667761165541,"user":{"displayName":"Chiranjeevi Konduru","userId":"10258871021540197644"},"user_tz":360},"id":"IjbsUkcpNK9W","outputId":"150a834f-b826-4724-ffe0-834ec747e622"},"outputs":[{"name":"stdout","output_type":"stream","text":["Encoder and Decoder models initialized!\n"]}],"source":["### DO NOT EDIT ###\n","\n","if __name__ == '__main__':\n","    # HYPERPARAMETERS - feel free to change\n","    LEARNING_RATE = 0.001\n","    HIDDEN_UNITS=256\n","    N_EPOCHS=10\n","\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","  \n","    rnn_encoder = RnnEncoder(src_vocab, EMBEDDING_DIM, HIDDEN_UNITS).to(device)\n","    rnn_decoder = RnnDecoder(trg_vocab, EMBEDDING_DIM, HIDDEN_UNITS).to(device)\n","\n","    rnn_model_params = list(rnn_encoder.parameters()) + list(rnn_decoder.parameters())\n","    optimizer = torch.optim.Adam(rnn_model_params, lr=LEARNING_RATE)\n","\n","    print('Encoder and Decoder models initialized!')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":493,"referenced_widgets":["239dda124426456b85694f296c97f43e","a837b4ed77934a0fbc1635b7fb355878","714f7aa59df64b7c85f56b171dc21819","6290747c4b86403c9c9da7700484e08b","9f3bd4cf01524670a917a2ac06028c88","df97998b31ba47ae993e8a1360398237","051afe5474c44b5c868c537f7bff80a4","ca6a3d45538c4ecc9510f73bf66b8287","3940d6e1237f4c36b7fd0daa00881ace","7238023739024bc280fb4cf7fa38e51c","17990007f8ac472a8794685bba7f67f4","2b97a66f249249f7b6678b9e623d347a","0244679fb5e945aab3d1438f135f604c","f57f45a822ea4a549a0900976cef626d","b932a2f2bd9948efabb9963ce1ec507d","4176c7a00f604d958aab0bc1b9a18d61","2452782a14a8467b81c3ec1952eab57a","e1d81d1ec0d14e25a32bba2544746f31","e2ecca5322da4b69a3a0640837a429e5","1f2f009e9123422eac636b7185e5e5bc","aac6f7b4a2cf4bb494384d24f4a7c192","81909bce33d04fc4bb63164cfe17fea6","c7744805ad6c4e1d8c72a7fb0f613239","2c56ccb755474dd5aa4da67cbb03eb53","1f50a03ed1fb4cdba8de197e8c1baf53","5d1efffaef7a4c008a58fccb2db521dc","1562625b6bab4479885677018a8ddfe9","9a49604021544772a4ae602e7c80fa73","38eeb63deb60493298c50a2ebfa94127","0e73f5fd8a3c452db544b32f31d92b58","6cfa6efa911e4db394ac28ce4a5e30bb","6dd5c10800b645c2ad2d9b4179190102","aa74b06872484e169992570798568ebf","dc95e7727196462b96ba30bc4791f953","5629719253f84b1a8577927b54d5f62a","a8b028d16c1c4c28b35e62e57ad41411","9f0e3e2991674b9c9e2c5eecd5521cc2","4881cad3472a4ea7b51d1a6056886cc3","e092174e8e264773871a898d9fc4d25b","7f2954833f9844c195d9e1255b50b482","c6ec17e84cdb427496154a5aebfc8f09","abf594b1d4494c13886c63ffb709377c","0ff8397601594b68b0d17ba35a4f37d5","e6f2045ec53a4c319090ad6832922e0b","2e379acac22b4ad4a93c3a52f4d2be75","1800533fabfa4879b79ab7c6eca67835","65c429d251594e5e8aad585b8efeb74c","077c4023a39b4c74b83b7f04f7c24b4a","bee9d72b6b0b46499b804db512f8bb0c","4a483abdcfa84628a7f57a4af4aa96b0","e4e65ec693ee4c1a899477b0d43289e5","2c14a2b9217e4bbabc13cf21bb994836","763a069d1b6049d983d63cedf1f1e4b8","e26b01b40de041798d84f4d474ca2a49","2ab5911f14b7470b9d84e2a8f6c61742","143a799d61ed4c51a82a80ac8c376301","41d10014575f4d8bb2a3b9e87e5d1185","994ef49b00304689877b3f3c0a69e397","e9ae609663864f1bb0822954ab462f40","f315cc71d0404c0e8b192d6c536cb457","48a393bef3ef48169dcdf08348c26008","94205b81f2734a7fa723c17c3039c69c","0d6ec13ee7124458ae7c1b627aba69a4","151c396c9619420d9ac72220d1b08b47","d3e71dd9684249c5a22ee42efe60171b","a1fdea7d6f9c4f88beca5f007b4361d2","e844c1cb1c114ec48d5b4b2c41b2a1e7","45c784d3cc5c4da19607f87f434d6639","fc00d61c091c4a05bf1d81a3c1ea04b4","bca4cd03641d4084a095f2fa161be586","13d68b2cf7d746089f3950f15415c0bf","2b67862cb2a64303be5b19104d486f1e","a437f6ed0aa842c29ca4dcc6746b0f0e","0b91e193018c474e9746eab260e4dc69","a2f44441f69249d8a15ad86e771bcfe5","6c06e9cdbfa344bba7603bd41c178ed1","8a3b268c41c54dbabdf8378e3c775114","05f47649e4d444f993416d60fcd1158a","f516c8753a4e4825a3dfe4aeeb41df69","14409ba9d27944d2a7533f8fc51e1c17","7f01b352942442919f3416b21fab5b8e","1c3bf5f50434405584c19110d5435dbc","8016868940b2406a90793aedf3ebd091","1590dc87d71945b2a76f8752931562de","8799efe939a54cce829696532ffcc737","61c6328f4f0a456a978593bf93bf5374","94cc2ac4c3f34b9ca95e9fab261b8502","624d74fa30b24b0dabfa16e44894282a","5a8d980c9dc949168eb9001601792d80","d2291ca69fa04963841b42513a70e200","73b50b70009a4c8aad0b267169eb240d","e2d8c453a1084cd4a625eee988ca46ba","7470c09d93404cfd9e8b01251b0e973d","d08391037342464f8332f086f4efb57e","ec86a55df49f4526b348b3f2154ecfe9","e638e2177232429ba63c93e6cdb873a2","ddca176f9058475493fb5a0d20f9511b","2f711a498b2d4fcbb607bc39d505b9bb","904b2f38a95e4deba95836b4a8c84c85","48b2ca1b8ca843c0aca5d8035e47d853","ffac8ebbb66646979a6e15aa58f0ef37","4682a42aba574b0ab3152967ad1e93d5","4a091893e2794b94b9e394c9505dfd66","7bb64d0fddba499b88df113d55204f47","dc4729a82d2c430a839d224a7574ea52","779538be6b4d4283974efeee02e662a4","899ff269a937476c85e65a6047c44902","2930b5d093ef49e78619d6cf4306346c","cdd22aa57df8492a91e5a7c836fcac40","2bb80c2b9cbc45b4a15cab8b49d4a5ca"]},"id":"OPosimvgdx_O","outputId":"7192337c-5105-4597-dbdc-9ac48504f773"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"239dda124426456b85694f296c97f43e","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/468 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: 1/10\t Loss: 1.8028 \t(270.76s)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2b97a66f249249f7b6678b9e623d347a","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/468 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: 2/10\t Loss: 1.2005 \t(269.03s)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c7744805ad6c4e1d8c72a7fb0f613239","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/468 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: 3/10\t Loss: 0.9229 \t(266.60s)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dc95e7727196462b96ba30bc4791f953","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/468 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: 4/10\t Loss: 0.7133 \t(266.15s)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2e379acac22b4ad4a93c3a52f4d2be75","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/468 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: 5/10\t Loss: 0.5446 \t(266.28s)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"143a799d61ed4c51a82a80ac8c376301","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/468 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: 6/10\t Loss: 0.4082 \t(265.91s)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e844c1cb1c114ec48d5b4b2c41b2a1e7","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/468 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: 7/10\t Loss: 0.3013 \t(263.89s)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"05f47649e4d444f993416d60fcd1158a","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/468 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: 8/10\t Loss: 0.2204 \t(263.34s)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5a8d980c9dc949168eb9001601792d80","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/468 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: 9/10\t Loss: 0.1613 \t(262.14s)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"48b2ca1b8ca843c0aca5d8035e47d853","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/468 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch:10/10\t Loss: 0.1197 \t(258.73s)\n","Model trained!\n"]}],"source":["### DO NOT EDIT ###\n","\n","if __name__ == '__main__':\n","    train_rnn_model(rnn_encoder, rnn_decoder, train_dataset, optimizer, trg_vocab, device, N_EPOCHS)"]},{"cell_type":"markdown","metadata":{"id":"M-t0mPoBq-D7"},"source":["## <font color='red'>TODO:</font> Inference (Decoding) Function [5 points]\n","\n","Now that we have trained the model, we can use it on test data.\n","\n","Here, you will write a function that takes your trained model and a source sentence (Spanish), and returns its translation (English sentence). Instead of using teacher forcing, the input to the decoder at time step $t_i$ will be the prediction of the decoder at time $t_{i-1}$."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"2RiLsOrtfzUb"},"outputs":[],"source":["def decode_rnn_model(encoder, decoder, src, max_decode_len, device):\n","    \"\"\"\n","    Args:\n","        encoder: Your RnnEncoder object\n","        decoder: Your RnnDecoder object\n","        src: [max_src_length, batch_size] the source sentences you wish to translate\n","        max_decode_len: The maximum desired length (int) of your target translated sentences\n","        device: the device your torch tensors are on (you may need to call x.to(device) for some of your tensors)\n","\n","    Returns:\n","        curr_output: [batch_size, max_decode_len] containing your predicted translated sentences\n","        curr_predictions: [batch_size, max_decode_len, trg_vocab_size] containing the (unnormalized) probabilities of each\n","            token in your vocabulary at each time step\n","\n","    Pseudo-code:\n","    - Obtain encoder output and hidden state by encoding src sentences\n","    - For 1 ≤ t ≤ max_decode_len:\n","        - Obtain your (unnormalized) prediction probabilities and hidden state by feeding dec_input (the best words \n","          from the previous time step), previous hidden state, and encoder output to decoder\n","        - Save your (unnormalized) prediction probabilities in curr_predictions at index t\n","        - Obtain your new dec_input by selecting the most likely (highest probability) token\n","        - Save dec_input in curr_output at index t\n","    \"\"\"\n","    # Initialize variables\n","    trg_vocab = decoder.trg_vocab\n","    batch_size = src.size(1)\n","    curr_output = torch.zeros((batch_size, max_decode_len))\n","    curr_predictions = torch.zeros((batch_size, max_decode_len, len(trg_vocab.idx2word)))\n","\n","    # We start the decoding with the start token for each example\n","    dec_input = torch.tensor([[trg_vocab.word2idx['<start>']]] * batch_size)\n","    curr_output[:, 0] = dec_input.squeeze(1)\n","    \n","    ### TODO: Implement decoding algorithm ###\n","    encoder_output , encoder_hs = encoder(src)\n","    hidden = encoder_hs\n","    encoder_output = encoder_output\n","\n","    dec_input = dec_input\n","\n","    for t in range(1, max_decode_len):\n","      output, hidden, attention_weights = decoder(dec_input, hidden, encoder_output)\n","\n","      curr_predictions[:,t,:] = output\n","      #Most likely token using argmax function\n","      dec_input = torch.argmax(output,1).unsqueeze(1)\n","      curr_output[:,t] = dec_input.squeeze(1)\n","  \n","    return curr_output, curr_predictions"]},{"cell_type":"markdown","metadata":{"id":"sZvb1C-qQzw4"},"source":["You can run the cell below to qualitatively compare some of the sentences your model generates with the some of the correct translations."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"TTgDbyKNPjRz","outputId":"11ae077e-1a00-4cc0-8e79-537ff50d8c89"},"outputs":[{"name":"stdout","output_type":"stream","text":["Source sentence: <start> ¿ como se puede parar esto ? <end>\n","Target sentence: <start> how can this be stopped ? <end>\n","Predicted sentence: <start> how can this be how can this be how can this\n","----------------\n","Source sentence: <start> si quieres , puedes ir . <end>\n","Target sentence: <start> if you want , you can go . <end>\n","Predicted sentence: <start> if you can go , you can go , you can\n","----------------\n","Source sentence: <start> no sabia que eso existia . <end>\n","Target sentence: <start> i didn t know it existed . <end>\n","Predicted sentence: <start> i didn t know it existed . <end>\n","----------------\n","Source sentence: <start> no me lastimaron . <end>\n","Target sentence: <start> they didn t hurt me . <end>\n","Predicted sentence: <start> they didn t hurt me . <end>\n","----------------\n","Source sentence: <start> ayer me encontre con mary . <end>\n","Target sentence: <start> i met mary yesterday . <end>\n","Predicted sentence: <start> i met mary yesterday . <end>\n","----------------\n"]}],"source":["### DO NOT EDIT ###\n","\n","if __name__ == '__main__':\n","    rnn_encoder.eval()\n","    rnn_decoder.eval()\n","    idxes = random.choices(range(len(test_dataset.dataset)), k=5)\n","    src, trg =  train_dataset.dataset[idxes]\n","    curr_output, _ = decode_rnn_model(rnn_encoder, rnn_decoder, src.transpose(0,1).to(device), trg.size(1), device)\n","    for i in range(len(src)):\n","        print(\"Source sentence:\", ' '.join([x for x in [src_vocab.idx2word[j.item()] for j in src[i]] if x != '<pad>']))\n","        print(\"Target sentence:\", ' '.join([x for x in [trg_vocab.idx2word[j.item()] for j in trg[i]] if x != '<pad>']))\n","        print(\"Predicted sentence:\", ' '.join([x for x in [trg_vocab.idx2word[j.item()] for j in curr_output[i]] if x != '<pad>']))\n","        print(\"----------------\")"]},{"cell_type":"markdown","metadata":{"id":"Vpr-psDzf5BV"},"source":["## Evaluate RNN Model [20 points]\n","\n","We provide you with a function to run the test set through the model and calculate BLEU scores. We expect your BLEU scores to satisfy the following conditions:  \n","\n","*   BLEU-1 > 0.290\n","*   BLEU-2 > 0.082\n","*   BLEU-3 > 0.060\n","*   BLEU-4 > 0.056\n","\n","Read more about Bleu Score at :\n","\n","1.   https://en.wikipedia.org/wiki/BLEU\n","2.   https://www.aclweb.org/anthology/P02-1040.pdf"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"bpLm8d1iq9bH"},"outputs":[],"source":["### DO NOT EDIT ###\n","\n","def get_reference_candidate(target, pred, trg_vocab):\n","    def _to_token(sentence):\n","        lis = []\n","        for s in sentence[1:]:\n","            x = trg_vocab.idx2word[s]\n","            if x == \"<end>\": break\n","            lis.append(x)\n","        return lis\n","    reference = _to_token(list(target.numpy()))\n","    candidate = _to_token(list(pred.numpy()))\n","    return reference, candidate\n","\n","def compute_bleu_scores(target_tensor_val, target_output, final_output, trg_vocab):\n","    bleu_1 = 0.0\n","    bleu_2 = 0.0\n","    bleu_3 = 0.0\n","    bleu_4 = 0.0\n","\n","    smoother = SmoothingFunction()\n","    save_reference = []\n","    save_candidate = []\n","    for i in range(len(target_tensor_val)):\n","        reference, candidate = get_reference_candidate(target_output[i], final_output[i], trg_vocab)\n","    \n","        bleu_1 += sentence_bleu(reference, candidate, weights=(1,), smoothing_function=smoother.method1)\n","        bleu_2 += sentence_bleu(reference, candidate, weights=(1/2, 1/2), smoothing_function=smoother.method1)\n","        bleu_3 += sentence_bleu(reference, candidate, weights=(1/3, 1/3, 1/3), smoothing_function=smoother.method1)\n","        bleu_4 += sentence_bleu(reference, candidate, weights=(1/4, 1/4, 1/4, 1/4), smoothing_function=smoother.method1)\n","\n","        save_reference.append(reference)\n","        save_candidate.append(candidate)\n","    \n","    bleu_1 = bleu_1/len(target_tensor_val)\n","    bleu_2 = bleu_2/len(target_tensor_val)\n","    bleu_3 = bleu_3/len(target_tensor_val)\n","    bleu_4 = bleu_4/len(target_tensor_val)\n","\n","    scores = {\"bleu_1\": bleu_1, \"bleu_2\": bleu_2, \"bleu_3\": bleu_3, \"bleu_4\": bleu_4}\n","    print('BLEU 1-gram: %f' % (bleu_1))\n","    print('BLEU 2-gram: %f' % (bleu_2))\n","    print('BLEU 3-gram: %f' % (bleu_3))\n","    print('BLEU 4-gram: %f' % (bleu_4))\n","\n","    return save_candidate, scores\n","\n","def evaluate_rnn_model(encoder, decoder, test_dataset, target_tensor_val, device):\n","    trg_vocab = decoder.trg_vocab\n","    batch_size = test_dataset.batch_size\n","    n_batch = 0\n","    total_loss = 0\n","\n","    encoder.eval()\n","    decoder.eval()\n","    \n","    final_output, target_output = None, None\n","\n","    with torch.no_grad():\n","        for batch, (src, trg) in enumerate(test_dataset):\n","            n_batch += 1\n","            loss = 0\n","            curr_output, curr_predictions = decode_rnn_model(encoder, decoder, src.transpose(0,1).to(device), trg.size(1), device)\n","            for t in range(1, trg.size(1)):\n","                loss += loss_function(trg[:, t].to(device), curr_predictions[:,t,:].to(device))\n","\n","            if final_output is None:\n","                final_output = torch.zeros((len(target_tensor_val), trg.size(1)))\n","                target_output = torch.zeros((len(target_tensor_val), trg.size(1)))\n","            final_output[batch*batch_size:(batch+1)*batch_size] = curr_output\n","            target_output[batch*batch_size:(batch+1)*batch_size] = trg\n","            batch_loss = (loss / int(trg.size(1)))\n","            total_loss += batch_loss\n","\n","        print('Loss {:.4f}'.format(total_loss / n_batch))\n","    \n","    # Compute BLEU scores\n","    return compute_bleu_scores(target_tensor_val, target_output, final_output, trg_vocab)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Doqxb5jnpk9z","outputId":"39535c78-f416-47f4-b328-44b1b997ecf6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loss 2.6023\n","BLEU 1-gram: 0.295339\n","BLEU 2-gram: 0.085789\n","BLEU 3-gram: 0.064210\n","BLEU 4-gram: 0.061466\n"]}],"source":["### DO NOT EDIT ###\n","\n","if __name__ == '__main__':\n","    rnn_save_candidate, rnn_scores = evaluate_rnn_model(rnn_encoder, rnn_decoder, test_dataset, trg_tensor_val, device)"]},{"cell_type":"markdown","metadata":{"id":"gZgsgEI8t0Jm"},"source":["# Step 3: Train a Transformer [45 points]\n","\n","Here you will write a transformer model for machine translation, and then train and evaluate its results. Here are some helpful links:\n","<ul>\n","<li> Original transformer paper: https://arxiv.org/pdf/1706.03762.pdf\n","<li> Helpful tutorial: http://jalammar.github.io/illustrated-transformer/\n","<li> Another tutorial: http://peterbloem.nl/blog/transformers\n","</ul>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"HT3k1qK5jc_i"},"outputs":[],"source":["### DO NOT EDIT ###\n","\n","import math"]},{"cell_type":"markdown","metadata":{"id":"PusbB7PNL0v3"},"source":["## <font color='red'>TODO:</font> Positional Embeddings [5 points]\n","\n","Similar to the RNN, we start with the Encoder model. A key component of the encoder is the Positional Embedding. As we know, word embeddings encode words in such a way that words with similar meaning have similar vectors. Because there are no recurrences in a Transformer, we need a way to tell the transformer the relative position of words in a sentence: so will add a positional embedding to the word embeddings. Now, two words with a similar embedding will both be close in meaning and occur near each other in the sentence.\n","\n","You will create a positional embedding matrix of size $(max\\_len, embed\\_dim)$ using the following formulae:\n","<br>\n","$\\begin{align*} pe[pos,2i] &= \\sin \\Big (\\frac{pos}{10000^{2i/embed\\_dim}}\\Big )\\\\pe[pos,2i+1] &= \\cos \\Big (\\frac{pos}{10000^{2i/embed\\_dim}}\\Big ) \\end{align*}$\n","\n","<font color='green'><b>Hint:</b> You should probably take the logarithm of the denominator to avoid raising $10000$ to an exponent and then exponentiate the result before plugging it into the fraction. This will help you avoid numerical (overflow/underflow) issues.\n","\n","<font color='green'><b>Hint:</b> We encourage you to try to implement this function with no for loops, which is the general practice (as it is faster). However, since we are using relatively small datasets, you are welcome to do this with for loops if you prefer."]},{"cell_type":"code","execution_count":91,"metadata":{"id":"FI1qLGOOdtYR","executionInfo":{"status":"ok","timestamp":1667768250358,"user_tz":360,"elapsed":710,"user":{"displayName":"Chiranjeevi Konduru","userId":"10258871021540197644"}}},"outputs":[],"source":["def create_positional_embedding(max_len, embed_dim):\n","    '''\n","    Args:\n","        max_len: The maximum length supported for positional embeddings\n","        embed_dim: The size of your embeddings\n","    Returns:\n","        pe: [max_len, 1, embed_dim] computed as in the formulae above\n","    '''\n","\n","    ### TODO ###\n","\n","    pe = torch.zeros(max_len, embed_dim)\n","    position = torch.arange(0, max_len).unsqueeze(1)\n","    d_term = torch.exp((torch.arange(0, embed_dim, 2, dtype=torch.float) *-(math.log(10000.0) / embed_dim)))\n","    pe[:, 0::2] = torch.sin(position.float() * d_term)\n","    pe[:, 1::2] = torch.cos(position.float() * d_term)\n","    pe = pe.unsqueeze(0).transpose(0,1)\n","\n","    return pe"]},{"cell_type":"markdown","metadata":{"id":"Q9juHL8hikXv"},"source":["## <font color='red'>TODO:</font> Encoder Model [10 points]\n","\n","Now you will create the Encoder model for the transformer.\n","\n","In this cell, you should implement the `__init(...)` and `forward(...)` functions, each of which is <b>5 points</b>."]},{"cell_type":"code","execution_count":92,"metadata":{"id":"0BVTSc8HtrjF","executionInfo":{"status":"ok","timestamp":1667768252698,"user_tz":360,"elapsed":258,"user":{"displayName":"Chiranjeevi Konduru","userId":"10258871021540197644"}}},"outputs":[],"source":["class TransformerEncoder(nn.Module):\n","    def __init__(self, src_vocab, embedding_dim, num_heads,\n","        num_layers, dim_feedforward, max_len_src, device):\n","        super(TransformerEncoder, self).__init__()\n","        self.device = device\n","        \"\"\"\n","        Args:\n","            src_vocab: Vocab_Lang, the source vocabulary\n","            embedding_dim: the dimension of the embedding (also the number of expected features for the input of the Transformer)\n","            num_heads: The number of attention heads\n","            num_layers: the number of Transformer Encoder layers\n","            dim_feedforward: the dimension of the feedforward network models in the Transformer\n","            max_len_src: maximum length of the source sentences\n","            device: the working device (you may need to map your postional embedding to this device)\n","        \"\"\"\n","        self.src_vocab = src_vocab # Do not change\n","        src_vocab_size = len(src_vocab)\n","\n","        # Create positional embedding matrix\n","        self.position_embedding = create_positional_embedding(max_len_src, embedding_dim).to(device)\n","        self.register_buffer('positional_embedding', self.position_embedding) # this informs the model that position_embedding is not a learnable parameter\n","\n","        ### TODO ###\n","        # Initialize embedding layer\n","        self.embedding = nn.Embedding(src_vocab_size,embedding_dim).to(self.device)\n","        \n","        # Dropout layer\n","        \n","        self.dropout = nn.Dropout().to(self.device)\n","        # Initialize a nn.TransformerEncoder model (you'll need to use embedding_dim, num_layers, num_heads, & dim_feedforward here)\n","        enc_layers = nn.TransformerEncoderLayer(embedding_dim, num_heads, dim_feedforward).to(self.device)\n","        self.trnsfmrEncoder = nn.TransformerEncoder(enc_layers, num_layers).to(self.device)\n","\n","    def make_src_mask(self, src):\n","        \"\"\"\n","        Args:\n","            src: [max_len, batch_size]\n","        Returns:\n","            Boolean matrix of size [batch_size, max_len] indicating which indices are padding\n","        \"\"\"\n","        assert len(src.shape) == 2, 'src must have exactly 2 dimensions'\n","        src_mask = src.transpose(0, 1) == 0 # padding idx\n","        return src_mask.to(self.device) # [batch_size, max_src_len]\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Args:\n","            x: [max_len, batch_size]\n","        Returns:\n","            output: [max_len, batch_size, embed_dim]\n","        Pseudo-code (note: x refers to the original input to this function throughout the pseudo-code):\n","        - Pass x through the word embedding\n","        - Add positional embedding to the word embedding, then apply dropout\n","        - Call make_src_mask(x) to compute a mask: this tells us which indexes in x\n","          are padding, which we want to ignore for the self-attention\n","        - Call the encoder, with src_key_padding_mask = src_mask\n","        \"\"\"\n","\n","        ### TODO ###\n","        embedding = self.embedding(x)\n","        position = (self.position_embedding[:x.size(0)]).to(self.device)\n","\n","        positional_embedding = (embedding + position).to(self.device)\n","        embedding = self.dropout(positional_embedding).to(self.device)\n","        \n","        embedding_masked = self.make_src_mask(x).to(self.device)\n","\n","        output = self.trnsfmrEncoder(embedding, src_key_padding_mask = embedding_masked).to(self.device)\n","\n","        return output     "]},{"cell_type":"markdown","metadata":{"id":"41Cx5tcfqyTq"},"source":["## Sanity Check: Transformer Encoder\n","\n","The code below runs a sanity check for your `TransformerEncoder` class. The tests are similar to the hidden ones in Gradescope. However, note that passing the sanity check does <b>not</b> guarantee that you will pass the autograder; it is intended to help you debug."]},{"cell_type":"code","execution_count":93,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xet_lf1rq0uR","executionInfo":{"status":"ok","timestamp":1667768256434,"user_tz":360,"elapsed":514,"user":{"displayName":"Chiranjeevi Konduru","userId":"10258871021540197644"}},"outputId":"d4c05e12-cbea-46d5-c487-39e964ae65ee"},"outputs":[{"output_type":"stream","name":"stdout","text":["--- TEST: Number of Model Parameters (tests __init__(...)) ---\n","\tPASSED\tInput: {'src_vocab': <__main__.Vocab_Lang object at 0x7f0d6ef39610>, 'embedding_dim': 4, 'num_heads': 1, 'dim_feedforward': 50, 'num_layers': 1, 'max_len_src': 10, 'device': device(type='cpu')}\tExpected Num. Params: 51890\tYour Num. Params: 51890\n","\tPASSED\tInput: {'src_vocab': <__main__.Vocab_Lang object at 0x7f0d6ef39610>, 'embedding_dim': 8, 'num_heads': 1, 'dim_feedforward': 50, 'num_layers': 1, 'max_len_src': 20, 'device': device(type='cpu')}\tExpected Num. Params: 103858\tYour Num. Params: 103858\n","\tPASSED\tInput: {'src_vocab': <__main__.Vocab_Lang object at 0x7f0d6ef39610>, 'embedding_dim': 12, 'num_heads': 1, 'dim_feedforward': 50, 'num_layers': 1, 'max_len_src': 30, 'device': device(type='cpu')}\tExpected Num. Params: 155954\tYour Num. Params: 155954\n","\tPASSED\tInput: {'src_vocab': <__main__.Vocab_Lang object at 0x7f0d6ef39610>, 'embedding_dim': 4, 'num_heads': 1, 'dim_feedforward': 100, 'num_layers': 2, 'max_len_src': 40, 'device': device(type='cpu')}\tExpected Num. Params: 53340\tYour Num. Params: 53340\n","\tPASSED\tInput: {'src_vocab': <__main__.Vocab_Lang object at 0x7f0d6ef39610>, 'embedding_dim': 8, 'num_heads': 2, 'dim_feedforward': 100, 'num_layers': 2, 'max_len_src': 50, 'device': device(type='cpu')}\tExpected Num. Params: 106736\tYour Num. Params: 106736\n","\tPASSED\tInput: {'src_vocab': <__main__.Vocab_Lang object at 0x7f0d6ef39610>, 'embedding_dim': 12, 'num_heads': 2, 'dim_feedforward': 100, 'num_layers': 2, 'max_len_src': 60, 'device': device(type='cpu')}\tExpected Num. Params: 160388\tYour Num. Params: 160388\n","\tPASSED\tInput: {'src_vocab': <__main__.Vocab_Lang object at 0x7f0d6ef39610>, 'embedding_dim': 4, 'num_heads': 2, 'dim_feedforward': 150, 'num_layers': 3, 'max_len_src': 70, 'device': device(type='cpu')}\tExpected Num. Params: 55690\tYour Num. Params: 55690\n","\tPASSED\tInput: {'src_vocab': <__main__.Vocab_Lang object at 0x7f0d6ef39610>, 'embedding_dim': 8, 'num_heads': 4, 'dim_feedforward': 150, 'num_layers': 3, 'max_len_src': 80, 'device': device(type='cpu')}\tExpected Num. Params: 111314\tYour Num. Params: 111314\n","\tPASSED\tInput: {'src_vocab': <__main__.Vocab_Lang object at 0x7f0d6ef39610>, 'embedding_dim': 12, 'num_heads': 4, 'dim_feedforward': 150, 'num_layers': 3, 'max_len_src': 90, 'device': device(type='cpu')}\tExpected Num. Params: 167322\tYour Num. Params: 167322\n"]}],"source":["### DO NOT EDIT ###\n","\n","if __name__==\"__main__\":\n","    # Set device\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    # Set random seed\n","    torch.manual_seed(42)\n","    # Create test inputs\n","    embedding_dim = [4, 8, 12]\n","    max_len = [10,20,30,40,50,60,70,80,90]\n","    num_layers = [1,1,1,2,2,2,3,3,3]\n","    nheads = [1, 1, 1, 1, 2, 2, 2, 4, 4]\n","    dimf = [50, 100, 150]\n","    params = []\n","    inputs = []\n","    i = 0\n","    for df in dimf:\n","        for ed in embedding_dim:\n","            inp = {}\n","            inp['src_vocab'] = src_vocab\n","            inp['embedding_dim'] = ed\n","            inp['num_heads'] = nheads[i]\n","            inp['dim_feedforward'] = df\n","            inp['num_layers'] = num_layers[i]\n","            inp['max_len_src'] = max_len[i]\n","            inp['device'] = device\n","            inputs.append(inp)\n","            i += 1\n","    # Test init\n","    expected_outputs = [51890, 103858, 155954, 53340, 106736, 160388, 55690, 111314, 167322]\n","\n","    sanityCheckModel(inputs, TransformerEncoder, expected_outputs, \"init\", None)"]},{"cell_type":"code","execution_count":94,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XQUWrgE-1Eo5","executionInfo":{"status":"ok","timestamp":1667768260250,"user_tz":360,"elapsed":960,"user":{"displayName":"Chiranjeevi Konduru","userId":"10258871021540197644"}},"outputId":"1850c463-ca82-4ebd-aa93-f4a72a998a3e"},"outputs":[{"output_type":"stream","name":"stdout","text":["--- TEST: Output shape of forward(...) ---\n","\tPASSED\t Init Input: {'src_vocab': <__main__.Vocab_Lang object at 0x7f0d6ef39610>, 'embedding_dim': 32, 'num_heads': 1, 'dim_feedforward': 100, 'num_layers': 1, 'max_len_src': 10, 'device': device(type='cpu')}\tForward Input Shape: torch.Size([1, 16])\tExpected Output Shape: torch.Size([1, 16, 32])\tYour Output Shape: torch.Size([1, 16, 32])\n","\tPASSED\t Init Input: {'src_vocab': <__main__.Vocab_Lang object at 0x7f0d6ef39610>, 'embedding_dim': 32, 'num_heads': 1, 'dim_feedforward': 100, 'num_layers': 1, 'max_len_src': 20, 'device': device(type='cpu')}\tForward Input Shape: torch.Size([2, 16])\tExpected Output Shape: torch.Size([2, 16, 32])\tYour Output Shape: torch.Size([2, 16, 32])\n","\tPASSED\t Init Input: {'src_vocab': <__main__.Vocab_Lang object at 0x7f0d6ef39610>, 'embedding_dim': 64, 'num_heads': 2, 'dim_feedforward': 100, 'num_layers': 2, 'max_len_src': 30, 'device': device(type='cpu')}\tForward Input Shape: torch.Size([1, 16])\tExpected Output Shape: torch.Size([1, 16, 64])\tYour Output Shape: torch.Size([1, 16, 64])\n","\tPASSED\t Init Input: {'src_vocab': <__main__.Vocab_Lang object at 0x7f0d6ef39610>, 'embedding_dim': 64, 'num_heads': 2, 'dim_feedforward': 100, 'num_layers': 2, 'max_len_src': 40, 'device': device(type='cpu')}\tForward Input Shape: torch.Size([2, 16])\tExpected Output Shape: torch.Size([2, 16, 64])\tYour Output Shape: torch.Size([2, 16, 64])\n","\tPASSED\t Init Input: {'src_vocab': <__main__.Vocab_Lang object at 0x7f0d6ef39610>, 'embedding_dim': 128, 'num_heads': 4, 'dim_feedforward': 100, 'num_layers': 3, 'max_len_src': 50, 'device': device(type='cpu')}\tForward Input Shape: torch.Size([1, 16])\tExpected Output Shape: torch.Size([1, 16, 128])\tYour Output Shape: torch.Size([1, 16, 128])\n","\tPASSED\t Init Input: {'src_vocab': <__main__.Vocab_Lang object at 0x7f0d6ef39610>, 'embedding_dim': 128, 'num_heads': 4, 'dim_feedforward': 100, 'num_layers': 3, 'max_len_src': 60, 'device': device(type='cpu')}\tForward Input Shape: torch.Size([2, 16])\tExpected Output Shape: torch.Size([2, 16, 128])\tYour Output Shape: torch.Size([2, 16, 128])\n"]}],"source":["### DO NOT EDIT ###\n","\n","if __name__==\"__main__\":\n","    # Set device\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    # Set random seed\n","    torch.manual_seed(42)\n","    # Test forward\n","    inputs = []\n","    batch_sizes = [1, 2]\n","    dimf = 100\n","    embedding_dims = [32,64,128]\n","    nheads = iter([1, 1, 2, 2, 4, 4])\n","    num_layers = iter([1,1,2,2,3,3])\n","    max_len = iter([10,20,30,40,50,60])\n","    for ed in embedding_dims:\n","        for b in batch_sizes:\n","            inp = {}\n","            inp['src_vocab'] = src_vocab\n","            inp['embedding_dim'] = ed\n","            inp['num_heads'] = next(nheads)\n","            inp['dim_feedforward'] = dimf\n","            inp['num_layers'] = next(num_layers)\n","            inp['max_len_src'] = next(max_len)\n","            inp['device'] = device\n","            inp[\"batch_size\"] = b\n","            inputs.append(inp)\n","    # create sanity datasets\n","    sanity_dataset = MyData(src_tensor_train, trg_tensor_train)\n","    sanity_loader = torch.utils.data.DataLoader(sanity_dataset, batch_size=50, num_workers=2, drop_last=True, shuffle=True)\n","    expected_outputs = [torch.Size([1, 16, 32]), torch.Size([2, 16, 32]), torch.Size([1, 16, 64]), torch.Size([2, 16, 64]), torch.Size([1, 16, 128]), torch.Size([2, 16, 128])]\n","\n","    sanityCheckModel(inputs, TransformerEncoder, expected_outputs, \"forward\", sanity_loader)"]},{"cell_type":"markdown","metadata":{"id":"CVCo7w4MMPKt"},"source":["## <font color='red'>TODO:</font> Decoder Model [10 points]\n","Now we implement a Decoder model. Unlike the RNN, you do not need to explicitly compute inter-attention with the encoder; you will use the nn.TransformerDecoder model, which takes care of this for you.\n","\n","In this cell, you should implement the `__init(...)` and `forward(...)` functions, each of which is <b>5 points</b>."]},{"cell_type":"code","execution_count":180,"metadata":{"executionInfo":{"elapsed":267,"status":"ok","timestamp":1667778387203,"user":{"displayName":"Chiranjeevi Konduru","userId":"10258871021540197644"},"user_tz":360},"id":"DJtTIHJYyNmU"},"outputs":[],"source":["class TransformerDecoder(nn.Module):\n","    def __init__(self, trg_vocab, embedding_dim, num_heads,\n","        num_layers, dim_feedforward, max_len_trg, device):\n","        super(TransformerDecoder, self).__init__()\n","        self.device = device\n","        \"\"\"\n","        Args:\n","            trg_vocab: Vocab_Lang, the target vocabulary\n","            embedding_dim: the dimension of the embedding (also the number of expected features for the input of the Transformer)\n","            num_heads: The number of attention heads\n","            num_layers: the number of Transformer Decoder layers\n","            dim_feedforward: the dimension of the feedforward network models in the Transformer\n","            max_len_trg: maximum length of the target sentences\n","            device: the working device (you may need to map your postional embedding to this device)\n","        \"\"\"\n","        self.trg_vocab = trg_vocab # Do not change\n","        trg_vocab_size = len(trg_vocab)\n","\n","        # Create positional embedding matrix\n","        self.position_embedding = create_positional_embedding(max_len_trg, embedding_dim).to(self.device)\n","        self.register_buffer('positional_embedding', self.position_embedding) # this informs the model that positional_embedding is not a learnable parameter\n","\n","        ### TODO ###\n","\n","        # Initialize embedding layer\n","        self.embedding = nn.Embedding(trg_vocab_size, embedding_dim).to(self.device)\n","\n","        # Dropout layer\n","        self.dropout_dec = nn.Dropout().to(self.device)\n","\n","        # Initialize a nn.TransformerDecoder model (you'll need to use embedding_dim, num_layers, num_heads, & dim_feedforward here)\n","        decoder_layer = nn.TransformerDecoderLayer(embedding_dim, num_heads, dim_feedforward).to(self.device)\n","        self.TrnsfrmrDecoder = nn.TransformerDecoder(decoder_layer,num_layers).to(self.device)\n","\n","        # Final fully connected layer\n","        self.fc = nn.Linear(embedding_dim,trg_vocab_size).to(self.device) \n","\n","    def generate_square_subsequent_mask(self, sz):\n","        \"\"\"Generate a square mask for the sequence. The masked positions are filled with float('-inf').\n","            Unmasked positions are filled with float(0.0).\n","        \"\"\"\n","        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n","        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0)).to(self.device)\n","        return mask\n","\n","    def forward(self, dec_in, enc_out):\n","        \"\"\"\n","        Args:\n","            dec_in: [sequence length, batch_size]\n","            enc_out: [max_len, batch_size, embed_dim]\n","        Returns:\n","            output: [sequence length, batch_size, trg_vocab_size]\n","        Pseudo-code:\n","        - Compute input word and positional embeddings in similar manner to encoder\n","        - Call generate_square_subsequent_mask() to compute a mask: this time,\n","          the mask is to prevent the decoder from attending to tokens in the \"future\".\n","          In other words, at time step i, the decoder should only attend to tokens\n","          1 to i-1.\n","        - Call the decoder, with tgt_mask = trg_mask\n","        - Run the output through the fully-connected layer and return it\n","        \"\"\"\n","        #Decoder input word embedding and it's positional embedding\n","        word_embedding = self.embedding(dec_in)\n","        position = (self.position_embedding[:dec_in.size(0)])\n","        positional_embedding = (word_embedding + position).to(self.device)\n","        #dropout\n","        new_embed = positional_embedding + self.dropout_dec(positional_embedding).to(self.device)\n","\n","        trg_mask = self.generate_square_subsequent_mask(dec_in.size(0)).to(self.device)\n","        output = self.TrnsfrmrDecoder(new_embed , enc_out, tgt_mask = trg_mask).to(self.device)\n","        output = self.fc(output).to(self.device)\n","\n","        ### TODO ###\n","\n","        return output    "]},{"cell_type":"markdown","metadata":{"id":"Vsdsm8jk2YHl"},"source":["## Sanity Check: Transformer Decoder\n","\n","The code below runs a sanity check for your `TransformerDecoder` class. The tests are similar to the hidden ones in Gradescope. However, note that passing the sanity check does <b>not</b> guarantee that you will pass the autograder; it is intended to help you debug."]},{"cell_type":"code","execution_count":182,"metadata":{"id":"1W093yUg25an","executionInfo":{"status":"ok","timestamp":1667778492988,"user_tz":360,"elapsed":246,"user":{"displayName":"Chiranjeevi Konduru","userId":"10258871021540197644"}}},"outputs":[],"source":["### DO NOT EDIT ###\n","\n","def sanityCheckTransformerDecoderModelForward(inputs, NN, expected_outputs):\n","    print('--- TEST: Output shape of forward(...) ---\\n')\n","    msg = ''\n","    for i, inp in enumerate(inputs):\n","        input_rep = '{'\n","        for k,v in inp.items():\n","            if torch.is_tensor(v):\n","                input_rep += str(k) + ': ' + 'Tensor with shape ' + str(v.size()) + ', '\n","            else:\n","                input_rep += str(k) + ': ' + str(v) + ', '\n","        input_rep += '}'\n","        dec = NN(trg_vocab=inp['trg_vocab'],embedding_dim=inp['embedding_dim'],num_heads=inp['num_heads'],num_layers=inp['num_layers'],dim_feedforward=inp['dim_feedforward'],max_len_trg=inp['max_len_trg'],device=inp['device'])\n","        dec_in = torch.randint(low=0,high=20,size=(inp['max_len_trg'], inp['batch_size']))\n","        enc_out = torch.rand(inp['max_len_trg'], inp['batch_size'], inp['embedding_dim'])\n","        inp['encoder_outputs'] = enc_out\n","        with torch.no_grad(): \n","            stu_out = dec(enc_out=enc_out, dec_in=dec_in)\n","        del dec\n","        has_passed = True\n","        if not torch.is_tensor(stu_out):\n","            has_passed = False\n","            msg = 'Output must be a torch.Tensor; received ' + str(type(stu_out))\n","        status = 'PASSED' if has_passed else 'FAILED'\n","        if not has_passed:\n","            message = '\\t' + status + \"\\t Init Input: \" + input_rep + '\\tForward Input Shape: ' + str(inp['encoder_outputs'].shape) + '\\tExpected Output Shape: ' + str(expected_outputs[i]) + '\\t' + msg\n","            print(message)\n","            continue\n","        \n","        has_passed = stu_out.size() == expected_outputs[i]\n","        msg = 'Your Output Shape: ' + str(stu_out.size())\n","        status = 'PASSED' if has_passed else 'FAILED'\n","        message = '\\t' + status + \"\\t Init Input: \" + input_rep + '\\tForward Input Shape: ' + str(inp['encoder_outputs'].shape) + '\\tExpected Output Shape: ' + str(expected_outputs[i]) + '\\t' + msg\n","        print(message)\n","        \n"]},{"cell_type":"code","execution_count":183,"metadata":{"id":"2dIb90hi3fC0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667778496828,"user_tz":360,"elapsed":1429,"user":{"displayName":"Chiranjeevi Konduru","userId":"10258871021540197644"}},"outputId":"62bcf8bd-2556-41b6-ae2e-d9b439ebab4d"},"outputs":[{"output_type":"stream","name":"stdout","text":["--- TEST: Number of Model Parameters (tests __init__(...)) ---\n","\tPASSED\tInput: {'trg_vocab': <__main__.Vocab_Lang object at 0x7f0d6ef397d0>, 'embedding_dim': 8, 'num_heads': 1, 'num_layers': 1, 'dim_feedforward': 50, 'max_len_trg': 64, 'device': device(type='cpu')}\tExpected Num. Params: 113835\tYour Num. Params: 113835\n","\tPASSED\tInput: {'trg_vocab': <__main__.Vocab_Lang object at 0x7f0d6ef397d0>, 'embedding_dim': 8, 'num_heads': 1, 'num_layers': 2, 'dim_feedforward': 50, 'max_len_trg': 64, 'device': device(type='cpu')}\tExpected Num. Params: 115317\tYour Num. Params: 115317\n","\tPASSED\tInput: {'trg_vocab': <__main__.Vocab_Lang object at 0x7f0d6ef397d0>, 'embedding_dim': 8, 'num_heads': 2, 'num_layers': 1, 'dim_feedforward': 50, 'max_len_trg': 64, 'device': device(type='cpu')}\tExpected Num. Params: 113835\tYour Num. Params: 113835\n","\tPASSED\tInput: {'trg_vocab': <__main__.Vocab_Lang object at 0x7f0d6ef397d0>, 'embedding_dim': 8, 'num_heads': 2, 'num_layers': 2, 'dim_feedforward': 50, 'max_len_trg': 64, 'device': device(type='cpu')}\tExpected Num. Params: 115317\tYour Num. Params: 115317\n","\tPASSED\tInput: {'trg_vocab': <__main__.Vocab_Lang object at 0x7f0d6ef397d0>, 'embedding_dim': 8, 'num_heads': 1, 'num_layers': 1, 'dim_feedforward': 100, 'max_len_trg': 64, 'device': device(type='cpu')}\tExpected Num. Params: 114685\tYour Num. Params: 114685\n","\tPASSED\tInput: {'trg_vocab': <__main__.Vocab_Lang object at 0x7f0d6ef397d0>, 'embedding_dim': 8, 'num_heads': 1, 'num_layers': 2, 'dim_feedforward': 100, 'max_len_trg': 64, 'device': device(type='cpu')}\tExpected Num. Params: 117017\tYour Num. Params: 117017\n","\tPASSED\tInput: {'trg_vocab': <__main__.Vocab_Lang object at 0x7f0d6ef397d0>, 'embedding_dim': 8, 'num_heads': 2, 'num_layers': 1, 'dim_feedforward': 100, 'max_len_trg': 64, 'device': device(type='cpu')}\tExpected Num. Params: 114685\tYour Num. Params: 114685\n","\tPASSED\tInput: {'trg_vocab': <__main__.Vocab_Lang object at 0x7f0d6ef397d0>, 'embedding_dim': 8, 'num_heads': 2, 'num_layers': 2, 'dim_feedforward': 100, 'max_len_trg': 64, 'device': device(type='cpu')}\tExpected Num. Params: 117017\tYour Num. Params: 117017\n","\n","--- TEST: Output shape of forward(...) ---\n","\n","\tPASSED\t Init Input: {embedding_dim: 100, trg_vocab: <__main__.Vocab_Lang object at 0x7f0d6ef397d0>, num_heads: 2, num_layers: 1, batch_size: 1, dim_feedforward: 50, max_len_trg: 16, device: cpu, }\tForward Input Shape: torch.Size([16, 1, 100])\tExpected Output Shape: torch.Size([16, 1, 6609])\tYour Output Shape: torch.Size([16, 1, 6609])\n","\tPASSED\t Init Input: {embedding_dim: 100, trg_vocab: <__main__.Vocab_Lang object at 0x7f0d6ef397d0>, num_heads: 2, num_layers: 1, batch_size: 2, dim_feedforward: 50, max_len_trg: 16, device: cpu, }\tForward Input Shape: torch.Size([16, 2, 100])\tExpected Output Shape: torch.Size([16, 2, 6609])\tYour Output Shape: torch.Size([16, 2, 6609])\n","\tPASSED\t Init Input: {embedding_dim: 200, trg_vocab: <__main__.Vocab_Lang object at 0x7f0d6ef397d0>, num_heads: 2, num_layers: 1, batch_size: 4, dim_feedforward: 50, max_len_trg: 16, device: cpu, }\tForward Input Shape: torch.Size([16, 4, 200])\tExpected Output Shape: torch.Size([16, 4, 6609])\tYour Output Shape: torch.Size([16, 4, 6609])\n","\tPASSED\t Init Input: {embedding_dim: 200, trg_vocab: <__main__.Vocab_Lang object at 0x7f0d6ef397d0>, num_heads: 2, num_layers: 1, batch_size: 1, dim_feedforward: 100, max_len_trg: 32, device: cpu, }\tForward Input Shape: torch.Size([32, 1, 200])\tExpected Output Shape: torch.Size([32, 1, 6609])\tYour Output Shape: torch.Size([32, 1, 6609])\n","\tPASSED\t Init Input: {embedding_dim: 200, trg_vocab: <__main__.Vocab_Lang object at 0x7f0d6ef397d0>, num_heads: 2, num_layers: 1, batch_size: 2, dim_feedforward: 100, max_len_trg: 32, device: cpu, }\tForward Input Shape: torch.Size([32, 2, 200])\tExpected Output Shape: torch.Size([32, 2, 6609])\tYour Output Shape: torch.Size([32, 2, 6609])\n","\tPASSED\t Init Input: {embedding_dim: 400, trg_vocab: <__main__.Vocab_Lang object at 0x7f0d6ef397d0>, num_heads: 2, num_layers: 1, batch_size: 4, dim_feedforward: 100, max_len_trg: 32, device: cpu, }\tForward Input Shape: torch.Size([32, 4, 400])\tExpected Output Shape: torch.Size([32, 4, 6609])\tYour Output Shape: torch.Size([32, 4, 6609])\n","\tPASSED\t Init Input: {embedding_dim: 400, trg_vocab: <__main__.Vocab_Lang object at 0x7f0d6ef397d0>, num_heads: 2, num_layers: 1, batch_size: 1, dim_feedforward: 200, max_len_trg: 64, device: cpu, }\tForward Input Shape: torch.Size([64, 1, 400])\tExpected Output Shape: torch.Size([64, 1, 6609])\tYour Output Shape: torch.Size([64, 1, 6609])\n","\tPASSED\t Init Input: {embedding_dim: 800, trg_vocab: <__main__.Vocab_Lang object at 0x7f0d6ef397d0>, num_heads: 2, num_layers: 1, batch_size: 2, dim_feedforward: 200, max_len_trg: 64, device: cpu, }\tForward Input Shape: torch.Size([64, 2, 800])\tExpected Output Shape: torch.Size([64, 2, 6609])\tYour Output Shape: torch.Size([64, 2, 6609])\n","\tPASSED\t Init Input: {embedding_dim: 800, trg_vocab: <__main__.Vocab_Lang object at 0x7f0d6ef397d0>, num_heads: 2, num_layers: 1, batch_size: 4, dim_feedforward: 200, max_len_trg: 128, device: cpu, }\tForward Input Shape: torch.Size([128, 4, 800])\tExpected Output Shape: torch.Size([128, 4, 6609])\tYour Output Shape: torch.Size([128, 4, 6609])\n"]}],"source":["### DO NOT EDIT ###\n","\n","if __name__ == '__main__':\n","    # Set device\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    # Set random seed\n","    torch.manual_seed(42)\n","    # Create test inputs\n","    hidden_units = [50, 100, 200]\n","    embedding_dim = [8, 16]\n","    num_heads = [1, 2]\n","    dim_feedforward = [50, 100]\n","    num_layers = [1, 2]\n","    max_lens = 64\n","    params = []\n","    inputs = []\n","    for ed in embedding_dim:\n","        for df in dim_feedforward:\n","            for nh in num_heads:\n","                for nl in num_layers:\n","                    inp = {}\n","                    inp['trg_vocab'] = trg_vocab\n","                    inp['embedding_dim'] = ed\n","                    inp['num_heads'] = nh\n","                    inp['num_layers'] = nl\n","                    inp['dim_feedforward'] = df\n","                    inp['max_len_trg'] = max_lens\n","                    inp['device'] = device\n","                    inputs.append(inp)\n","    # Test init\n","    expected_outputs = [113835, 115317, 113835, 115317, 114685, 117017, 114685, 117017]\n","    sanityCheckModel(inputs, TransformerDecoder, expected_outputs, \"init\", None)\n","    print()\n","\n","    # Test forward\n","    inputs = []\n","    batch_sizes = [1, 2, 4]\n","    num_heads = 2\n","    num_layers = 1\n","    embedding_dims = iter([100, 100, 200, 200, 200, 400, 400, 800, 800])\n","    expected_outputs = [torch.Size([16, 1, 6609]),torch.Size([16, 2, 6609]),torch.Size([16, 4, 6609]),torch.Size([32, 1, 6609]),torch.Size([32, 2, 6609]),torch.Size([32, 4, 6609]),torch.Size([64, 1, 6609]),torch.Size([64, 2, 6609]),torch.Size([128, 4, 6609])]\n","    max_lens = iter([16, 16, 16, 32, 32, 32, 64, 64, 128])\n","\n","    for hu in hidden_units:\n","        for b in batch_sizes:\n","            inp = {}\n","            edim = next(embedding_dims)\n","            inp['embedding_dim'] = edim\n","            inp['trg_vocab'] = trg_vocab\n","            inp['num_heads'] = num_heads\n","            inp['num_layers'] = num_layers\n","            inp[\"batch_size\"] = b\n","            inp['dim_feedforward'] = hu\n","            inp['max_len_trg'] = next(max_lens)\n","            inp['device'] = device\n","            inputs.append(inp)\n","    \n","    sanityCheckTransformerDecoderModelForward(inputs, TransformerDecoder, expected_outputs)\n"]},{"cell_type":"markdown","metadata":{"id":"g3zXR9gYMgek"},"source":["## Train Transformer Model\n","\n","Like the RNN, we train the encoder and decoder using cross-entropy loss."]},{"cell_type":"code","execution_count":184,"metadata":{"id":"dN6rnn4N3vcc","executionInfo":{"status":"ok","timestamp":1667778502832,"user_tz":360,"elapsed":246,"user":{"displayName":"Chiranjeevi Konduru","userId":"10258871021540197644"}}},"outputs":[],"source":["### DO NOT EDIT ###\n","\n","def train_transformer_model(encoder, decoder, dataset, optimizer, device, n_epochs):\n","    encoder.train()\n","    decoder.train()\n","    criterion = nn.CrossEntropyLoss(ignore_index=0)\n","    for epoch in range(n_epochs):\n","        start = time.time()\n","        losses = []\n","\n","        for src, trg in tqdm(train_dataset):\n","            \n","            src = src.to(device).transpose(0,1) # [max_src_length, batch_size]\n","            trg = trg.to(device).transpose(0,1) # [max_trg_length, batch_size]\n","\n","            enc_out = encoder(src)\n","            output = decoder(trg[:-1, :], enc_out)\n","\n","            output = output.reshape(-1, output.shape[2])\n","            trg = trg[1:].reshape(-1)\n","\n","            optimizer.zero_grad()\n","\n","            loss = criterion(output, trg)\n","            losses.append(loss.item())\n","\n","            loss.backward()\n","\n","            # Clip to avoid exploding grading issues\n","            torch.nn.utils.clip_grad_norm_(encoder.parameters(), max_norm=1)\n","            torch.nn.utils.clip_grad_norm_(decoder.parameters(), max_norm=1)\n","\n","            optimizer.step()\n","\n","        mean_loss = sum(losses) / len(losses)\n","        print('Epoch:{:2d}/{}\\t Loss:{:.4f} ({:.2f}s)'.format(epoch + 1, n_epochs, mean_loss, time.time() - start))\n"]},{"cell_type":"code","execution_count":185,"metadata":{"id":"B15xUHct1fuw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667778505160,"user_tz":360,"elapsed":256,"user":{"displayName":"Chiranjeevi Konduru","userId":"10258871021540197644"}},"outputId":"db945ca7-e2d0-4305-b30b-b472698819fe"},"outputs":[{"output_type":"stream","name":"stdout","text":["Encoder and Decoder models initialized!\n"]}],"source":["### DO NOT EDIT ###\n","\n","if __name__ == '__main__':\n","    # HYPERPARAMETERS - feel free to change\n","    LEARNING_RATE = 0.001\n","    DIM_FEEDFORWARD=512\n","    N_EPOCHS=10\n","    N_HEADS=2\n","    N_LAYERS=2\n","\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","    transformer_encoder = TransformerEncoder(src_vocab, EMBEDDING_DIM, N_HEADS, \n","                                 N_LAYERS,DIM_FEEDFORWARD,\n","                                 max_length_src, device).to(device)\n","    transformer_decoder = TransformerDecoder(trg_vocab, EMBEDDING_DIM, N_HEADS, \n","                              N_LAYERS,DIM_FEEDFORWARD,\n","                              max_length_trg, device).to(device)\n","\n","    transformer_model_params = list(transformer_encoder.parameters()) + list(transformer_decoder.parameters())\n","    optimizer = torch.optim.Adam(transformer_model_params, lr=LEARNING_RATE)\n","\n","    print('Encoder and Decoder models initialized!')"]},{"cell_type":"code","execution_count":186,"metadata":{"id":"ur0ucStn3vi5","colab":{"base_uri":"https://localhost:8080/","height":519,"referenced_widgets":["7ee7fc9855c84dd38e7763621fe976d6","90642b9f04784e8caa493274e7168c86","a00c82f11f3f4cb7857cc7bba00e760f","616e503b11674b53adf417798c574b51","b624274833b946a58410435cf3d90176","328083add2cb4e47a76cd82de0309684","beebe7ff6aff477ca331016019ce7206","4833a2d094ef4eeeaf18b11e88e442bc","422f9f9c11b14b5e9a181d6e5d92427f","af218bc723704cedbfa72303e839c7e3","3cfaaad927414ea2a5351a48b83e2ef7","463d43f7a14c4b79a1446b2219c3b9b3","549a46686beb4f87a12107b08204f914","a160db07ab1e409eafb63d0136ebb5e8","f984dabaa4574ef290157fcf5027fee6","b3e0cd7042234abab721e227104f952e","69c64a8c288f464c9b9e33b63b3607b8","2384e30d037e429aa8390b0778c40818","d150c8d90e6840bb8c7456dad8330c33","cec11e5768c2412e82c96e7f4fb5470a","a84e395b605f4438b31ee1b8b72ea2cc","cf6bc2d4316340a29242f18ff704f9b2","cfa3cf9832664ab68d635af2e3e70abd","1b649ffe17734c95bcfbfde5299ac9b1","bd50cf46137d416db7567858b7d4d457","0edfc1e141754cfc8e9b4948133d9167","a2e3e2b9b5804dcb8c3972f90c02e605","1863d161dd5b42f897f67c5488391bd6","b1c2ad2ff892415e99406a442e245278","de83661990f9432b9fccf33f78d4a17b","3fa2ad5fbb794b439648cab271c89fa2","7137e8d10ab94740859e62be55b41043","d619d6ab2cbe4d4680d6746bac9c1c32","7016df46c00c406f961339721fe3c717","49f449a8930a4a7199ddd3711fb404c7","a38cb53c338d42798e291dda2d010238","918f3cef52904a97a4061c59b223709a","d26cdf3cccaa4757a40804cf0365cf7f","911f5d82a0e44d9e95d32b3aa7a911c0","5e3b933856c3464ca7ea10b7332bfb94","849882c879334116a2d03d26b89845ee","85b0ad2bf114459d8454032abce5d190","bef333d303b542b0abae977620f4af67","eb5a177384a3498b84c6c7290283ca7f","f161f32f76194d5aa7e6ee87d818e3d6","e69c0c458e8f432cbae236d19a29f085","ae44d8c8fead4673abc63ed13dc80459","c12d9206a37c43b3bfdd78f44357eb08","1430cc1101064cc6acca731270a3ec7e","fe4693c8fc344288bca1d1c9be65a915","7b1c77fa858b4a41a78092e223c32566","f4e0819d6dac42489d2146183bc6ed88","9f7013a518cb4865a8f03e16e60813b6","10399975f60b4df78dab905b15909b24","a69fa222ab894644af450c43654f7c71","6deef8d9e58547ae85034c08d257c5bc","a7108a0a89c941ef8559588e8150b908","8dda673f6df249c59998d71931f3d34a","a6d7fbc3e51946e59b948f900e50bb37","059964edea5248f7be197cd180b2e706","18a8df6c699c45b3af3239970a57093d","6d5833e0dcdc4c4d9cd42eac8e2c1c57","6431372503db4eee982ea2defd8df27c","02316d57d4e94ca395540d35b7d8d9b0","baaee8df989845c6a4cb588d82741061","d6dab98868c74a7289f37f16fab408ed","88c21610bc084d1ca2f538d5d812c806","1682d8b2a4964335808f4fc83cf064cc","7572dc7bb77a4d5fa7d5d1c2cac4ee4a","7511a0995867456191a5d8eb688d778b","0afe963dc1284c60a1c18becb042a87a","8d92a1f7e4134173a3488714b2924ace","d527b5d90cee42698d8dd4e5f121f144","ca5b7b901c83463db2979dd406db3f67","3f3326f8bca940fba05249f65aea88c8","583ef6d228fa4f97b6173641bd24148e","7b34b323475b451fb2857a952d7598e0","2204855fb0e04a229fa435767922fa77","7a28e244546d485c921682d87357e5cc","617dcbc454224305b624214a40b0c5eb","fd706e5c2ae04c558c8eecdfcd2f7a95","c410ff08a43743f0b7bf0c75e618e710","e10b77fab9e64b3fbd9372964f851b26","2919fafaa4a7439a9ac74f5ac32cf7a9","86d790d84c8d4654868219add9e07e50","c4423fd84d7640f19333bd764c5e4ea8","a0efd20f3396424e96d58d61e59c7442","51b49435652b4442bb75ad64088e16eb","104e84ce46fa4abf97e0e8588b5c5ffa","b1b20181b0364ed09c01a5a4616cdeb9","c8aff08af2014f00b53849f9b0be3d1d","6a7f605bdd2347ecaf5a79510c1f738f","ae8a495d8759472db88281f529000983","e0848a9dd81b4d8d8204b2ac873b54b6","6724b4953ca14088b60adb41608b7a7f","fdbcccc24872405192bd7d6578941fdb","00da45ab29da4097929150cd3a3a18e9","7c258f8e269b471db37e58cfa28b2056","a3b7b32f6e9e40ab8b00e87641a29081","9b2c7035270540cfbf1878c5b2c8ff6a","d7ff01a223f843c3ad73aeb2f22258c2","f1d8af5ed5d249e985e3d1da9dfed334","aa502697e5dd4ada9faf2b2e459d3831","e0097b4f83bf484bb8c43a573a785bd8","dc113608c7164d0a996c23c4c83a5502","4670c20c63664504be259ce007f40d4d","a2cd6b784ac942b88a51a31bea8353b4","049d64d493904c578448beef35bf02b8","a31e21b6904c45c7916210a3a82b5fe5","8f2532f1c4e941409c1d3023a52acbab"]},"executionInfo":{"status":"ok","timestamp":1667781785140,"user_tz":360,"elapsed":3277013,"user":{"displayName":"Chiranjeevi Konduru","userId":"10258871021540197644"}},"outputId":"c55f6ab7-2bd5-44aa-de99-2c3172c6adf9"},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/468 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ee7fc9855c84dd38e7763621fe976d6"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch: 1/10\t Loss:3.2529 (283.85s)\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/468 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"463d43f7a14c4b79a1446b2219c3b9b3"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch: 2/10\t Loss:2.3829 (324.91s)\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/468 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cfa3cf9832664ab68d635af2e3e70abd"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch: 3/10\t Loss:2.0150 (363.65s)\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/468 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7016df46c00c406f961339721fe3c717"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch: 4/10\t Loss:1.7336 (372.21s)\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/468 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f161f32f76194d5aa7e6ee87d818e3d6"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch: 5/10\t Loss:1.5172 (379.70s)\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/468 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6deef8d9e58547ae85034c08d257c5bc"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch: 6/10\t Loss:1.3563 (364.27s)\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/468 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88c21610bc084d1ca2f538d5d812c806"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch: 7/10\t Loss:1.2252 (300.27s)\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/468 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2204855fb0e04a229fa435767922fa77"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch: 8/10\t Loss:1.1233 (294.68s)\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/468 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"104e84ce46fa4abf97e0e8588b5c5ffa"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch: 9/10\t Loss:1.0433 (295.34s)\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/468 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b2c7035270540cfbf1878c5b2c8ff6a"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch:10/10\t Loss:0.9690 (297.73s)\n"]}],"source":["### DO NOT EDIT ###\n","\n","if __name__ == '__main__':\n","    train_transformer_model(transformer_encoder, transformer_decoder, train_dataset, optimizer, device, N_EPOCHS)"]},{"cell_type":"markdown","metadata":{"id":"LbwJp2AMM0bz"},"source":["## <font color='red'>TODO:</font> Inference (Decoding) Function [5 points]\n","\n","Now that we have trained the model, we can use it on test data.\n","\n","Here, you will write a function that takes your trained transformer model and a source sentence (Spanish), and returns its translation (English sentence). Like the RNN, we use the prediction of the decoder as the input to the decoder for the sequence of outputs. For the RNN, at time step $t_i$ the decoder takes the hidden state $h_{i-1}$ and the previous prediction $w_{i-1}$ at each time step. However, because the transformer does not use recurrences, we do not pass a hidden state; instead, at time step $t_i$ we pass $w_1,w_2 \\cdots w_{i-1}$, which is the entire sequence predicted so far."]},{"cell_type":"code","execution_count":195,"metadata":{"id":"AOQh6Cfzohi1","executionInfo":{"status":"ok","timestamp":1667782210811,"user_tz":360,"elapsed":359,"user":{"displayName":"Chiranjeevi Konduru","userId":"10258871021540197644"}}},"outputs":[],"source":["def decode_transformer_model(encoder, decoder, src, max_decode_len, device):\n","    \"\"\"\n","    Args:\n","        encoder: Your TransformerEncoder object\n","        decoder: Your TransformerDecoder object\n","        src: [max_src_length, batch_size] the source sentences you wish to translate\n","        max_decode_len: The maximum desired length (int) of your target translated sentences\n","        device: the device your torch tensors are on (you may need to call x.to(device) for some of your tensors)\n","\n","    Returns:\n","        curr_output: [batch_size, max_decode_len] containing your predicted translated sentences\n","        curr_predictions: [batch_size, max_decode_len, trg_vocab_size] containing the (unnormalized) probabilities of each\n","            token in your vocabulary at each time step\n","\n","    Pseudo-code:\n","    - Obtain encoder output by encoding src sentences\n","    - For 1 ≤ t ≤ max_decode_len:\n","        - Obtain dec_input as the best words so far for previous time steps (you can get this from curr_output)\n","        - Obtain your (unnormalized) prediction probabilities by feeding dec_input and encoder output to decoder\n","        - Save your (unnormalized) prediction probabilities in curr_predictions at index t\n","        - Calculate the most likely (highest probability) token and save in curr_output at timestep t\n","    \"\"\"\n","    # Initialize variables\n","    trg_vocab = decoder.trg_vocab\n","    batch_size = src.size(1)\n","    curr_output = torch.zeros((batch_size, max_decode_len))\n","    curr_predictions = torch.zeros((batch_size, max_decode_len, len(trg_vocab.idx2word)))\n","    enc_output = None\n","\n","    # We start the decoding with the start token for each example\n","    dec_input = torch.tensor([[trg_vocab.word2idx['<start>']]] * batch_size).transpose(0,1)\n","    curr_output[:, 0] = dec_input.squeeze(1)\n","    \n","    ### TODO: Implement decoding algorithm ###\n","    enc_output = encoder(src)\n","\n","    for t in range(0,max_decode_len-1):\n","      dec_input = curr_output[:,:t+1]\n","      dec_input = dec_input.transpose(0,1)\n","      pred_prob = decoder(dec_input.to(torch.int32), enc_output)[-1]\n","\n","      curr_predictions[:,t+1,:] = pred_prob\n","      curr_output[:,t+1] = torch.argmax(pred_prob, dim =1) \n","       \n","    return curr_output, curr_predictions, enc_output"]},{"cell_type":"markdown","metadata":{"id":"shuxRQ_EWfGa"},"source":["You can run the cell below to qualitatively compare some of the sentences your model generates with the some of the correct translations."]},{"cell_type":"code","execution_count":196,"metadata":{"id":"yv3OYDbaWfGd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667782213381,"user_tz":360,"elapsed":376,"user":{"displayName":"Chiranjeevi Konduru","userId":"10258871021540197644"}},"outputId":"7380f30f-c3b1-4d32-9c59-1249d8844808"},"outputs":[{"output_type":"stream","name":"stdout","text":["Source sentence: <start> acabo de darme una ducha . <end>\n","Target sentence: <start> i just took a shower . <end>\n","Predicted sentence: <start> i just had a shower . <end> . <end> . <end>\n","----------------\n","Source sentence: <start> estos regalos son para ti . <end>\n","Target sentence: <start> these gifts are for you . <end>\n","Predicted sentence: <start> these gifts are for you . <end> . <end> . <end>\n","----------------\n","Source sentence: <start> esto va a la perfeccion . <end>\n","Target sentence: <start> this is going perfectly . <end>\n","Predicted sentence: <start> this will believe goes . <end> . <end> . <end> .\n","----------------\n","Source sentence: <start> estoy embarazada . <end>\n","Target sentence: <start> i m pregnant . <end>\n","Predicted sentence: <start> i m pregnant . <end> . <end> . <end> . <end>\n","----------------\n","Source sentence: <start> de acuerdo . <end>\n","Target sentence: <start> i agree . <end>\n","Predicted sentence: <start> i remember . <end> . <end> . <end> . <end> .\n","----------------\n"]}],"source":["### DO NOT EDIT ###\n","\n","if __name__ == '__main__':\n","    transformer_encoder.eval()\n","    transformer_decoder.eval()\n","    idxes = random.choices(range(len(test_dataset.dataset)), k=5)\n","    src, trg =  train_dataset.dataset[idxes]\n","    curr_output, _, _ = decode_transformer_model(transformer_encoder, transformer_decoder, src.transpose(0,1).to(device), trg.size(1), device)\n","    for i in range(len(src)):\n","        print(\"Source sentence:\", ' '.join([x for x in [src_vocab.idx2word[j.item()] for j in src[i]] if x != '<pad>']))\n","        print(\"Target sentence:\", ' '.join([x for x in [trg_vocab.idx2word[j.item()] for j in trg[i]] if x != '<pad>']))\n","        print(\"Predicted sentence:\", ' '.join([x for x in [trg_vocab.idx2word[j.item()] for j in curr_output[i]] if x != '<pad>']))\n","        print(\"----------------\")"]},{"cell_type":"markdown","metadata":{"id":"W3KeBgqjoVP-"},"source":["## Evaluate Transformer Model [20 points]\n","\n","Now we can run the test set through the transformer model. We expect your BLEU scores to satisfy the following conditions: \n","\n","*   BLEU-1 > 0.290\n","*   BLEU-2 > 0.082\n","*   BLEU-3 > 0.060\n","*   BLEU-4 > 0.056\n"]},{"cell_type":"code","execution_count":189,"metadata":{"id":"zjmkIfLzs0Ne","executionInfo":{"status":"ok","timestamp":1667781821718,"user_tz":360,"elapsed":318,"user":{"displayName":"Chiranjeevi Konduru","userId":"10258871021540197644"}}},"outputs":[],"source":["### DO NOT EDIT ###\n","\n","def evaluate_model(encoder, decoder, test_dataset, target_tensor_val, device):\n","    trg_vocab = decoder.trg_vocab\n","    batch_size = test_dataset.batch_size\n","    n_batch = 0\n","    total_loss = 0\n","\n","    encoder.eval()\n","    decoder.eval()\n","    criterion = nn.CrossEntropyLoss(ignore_index=0)\n","\n","    losses=[]\n","    final_output, target_output = None, None\n","\n","    with torch.no_grad():\n","        for batch, (src, trg) in enumerate(test_dataset):\n","            n_batch += 1\n","            loss = 0\n","            \n","            src, trg = src.transpose(0,1).to(device), trg.transpose(0,1).to(device)\n","            curr_output, curr_predictions, enc_out = decode_transformer_model(encoder, decoder, src, trg.size(0), device)\n","\n","            for t in range(1, trg.size(0)):\n","                output = decoder(trg[:-1, :], enc_out)\n","                output = output.reshape(-1, output.shape[2])\n","                loss_trg = trg[1:].reshape(-1)\n","                loss += criterion(output, loss_trg)\n","                # loss += criterion(curr_predictions[:,t,:].to(device), trg[t,:].reshape(-1).to(device))\n","\n","            if final_output is None:\n","                final_output = torch.zeros((len(target_tensor_val), trg.size(0)))\n","                target_output = torch.zeros((len(target_tensor_val), trg.size(0)))\n","\n","            final_output[batch*batch_size:(batch+1)*batch_size] = curr_output\n","            target_output[batch*batch_size:(batch+1)*batch_size] = trg.transpose(0,1)\n","            losses.append(loss.item() / (trg.size(0)-1))\n","\n","        mean_loss = sum(losses) / len(losses)\n","        print('Loss {:.4f}'.format(mean_loss))\n","    \n","    # Compute Bleu scores\n","    return compute_bleu_scores(target_tensor_val, target_output, final_output, trg_vocab)"]},{"cell_type":"code","execution_count":190,"metadata":{"id":"UaIW7SmMEBZ_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667782145216,"user_tz":360,"elapsed":312569,"user":{"displayName":"Chiranjeevi Konduru","userId":"10258871021540197644"}},"outputId":"07536bf1-967d-49fa-a503-5501360efc4c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loss 1.4280\n","BLEU 1-gram: 0.298153\n","BLEU 2-gram: 0.083671\n","BLEU 3-gram: 0.061409\n","BLEU 4-gram: 0.058573\n"]}],"source":["### DO NOT EDIT ###\n","\n","if __name__ == '__main__':\n","    transformer_save_candidate, transformer_scores = evaluate_model(transformer_encoder, transformer_decoder, test_dataset, trg_tensor_val, device)"]},{"cell_type":"markdown","metadata":{"id":"nYKSWRO93Mjz"},"source":["# What to Submit\n","\n","To submit the assignment, download this notebook as a <TT>.py</TT> file. You can do this by going to <TT>File > Download > Download .py</TT>. Then rename it to `hwk3.py`.\n","\n","You will also need to save the `rnn_encoder`, `rnn_decoder`, `transformer_encoder` and `transformer_decoder`. You can run the cell below to do this. After you save the files to your Google Drive, you need to manually download the files to your computer, and then submit them to the autograder.\n","\n","You will submit the following files to the autograder:\n","1.   `hwk3.py`, the download of this notebook as a `.py` file (**not** a `.ipynb` file)\n","1.   `rnn_encoder.pt`, the saved version of your `rnn_encoder`\n","1.   `rnn_decoder.pt`, the saved version of your `rnn_decoder`\n","1.   `transformer_encoder.pt`, the saved version of your `transformer_encoder`\n","1.   `transformer_decoder.pt`, the saved version of your `transformer_decoder`"]},{"cell_type":"code","execution_count":199,"metadata":{"id":"3HbvhiamQKSQ","executionInfo":{"status":"ok","timestamp":1667783119938,"user_tz":360,"elapsed":271,"user":{"displayName":"Chiranjeevi Konduru","userId":"10258871021540197644"}}},"outputs":[],"source":["### DO NOT EDIT ###\n","\n","import pickle"]},{"cell_type":"code","execution_count":200,"metadata":{"id":"R2Jnt32ItMA7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667783122768,"user_tz":360,"elapsed":1515,"user":{"displayName":"Chiranjeevi Konduru","userId":"10258871021540197644"}},"outputId":"bcff0aef-81e2-4e4b-d038-0b4b09d59d1d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","\n","Saving RNN model....\n","Saving Transformer model....\n"]}],"source":["### DO NOT EDIT ###\n","\n","if __name__=='__main__':\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    print()\n","    if rnn_encoder is not None and rnn_encoder is not None:\n","        print(\"Saving RNN model....\") \n","        torch.save(rnn_encoder, 'drive/My Drive/rnn_encoder.pt')\n","        torch.save(rnn_decoder, 'drive/My Drive/rnn_decoder.pt')\n","    if transformer_encoder is not None and transformer_decoder is not None:\n","        print(\"Saving Transformer model....\") \n","        torch.save(transformer_encoder, 'drive/My Drive/transformer_encoder.pt')\n","        torch.save(transformer_decoder, 'drive/My Drive/transformer_decoder.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8isQc_VRYeJu"},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[],"toc_visible":true},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0244679fb5e945aab3d1438f135f604c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2452782a14a8467b81c3ec1952eab57a","placeholder":"​","style":"IPY_MODEL_e1d81d1ec0d14e25a32bba2544746f31","value":"100%"}},"051afe5474c44b5c868c537f7bff80a4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"05f47649e4d444f993416d60fcd1158a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f516c8753a4e4825a3dfe4aeeb41df69","IPY_MODEL_14409ba9d27944d2a7533f8fc51e1c17","IPY_MODEL_7f01b352942442919f3416b21fab5b8e"],"layout":"IPY_MODEL_1c3bf5f50434405584c19110d5435dbc"}},"077c4023a39b4c74b83b7f04f7c24b4a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e26b01b40de041798d84f4d474ca2a49","placeholder":"​","style":"IPY_MODEL_2ab5911f14b7470b9d84e2a8f6c61742","value":" 468/468 [04:26&lt;00:00,  1.78it/s]"}},"0b91e193018c474e9746eab260e4dc69":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0d6ec13ee7124458ae7c1b627aba69a4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0e73f5fd8a3c452db544b32f31d92b58":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0ff8397601594b68b0d17ba35a4f37d5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13d68b2cf7d746089f3950f15415c0bf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"143a799d61ed4c51a82a80ac8c376301":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_41d10014575f4d8bb2a3b9e87e5d1185","IPY_MODEL_994ef49b00304689877b3f3c0a69e397","IPY_MODEL_e9ae609663864f1bb0822954ab462f40"],"layout":"IPY_MODEL_f315cc71d0404c0e8b192d6c536cb457"}},"14409ba9d27944d2a7533f8fc51e1c17":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8799efe939a54cce829696532ffcc737","max":468,"min":0,"orientation":"horizontal","style":"IPY_MODEL_61c6328f4f0a456a978593bf93bf5374","value":468}},"151c396c9619420d9ac72220d1b08b47":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1562625b6bab4479885677018a8ddfe9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1590dc87d71945b2a76f8752931562de":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"17990007f8ac472a8794685bba7f67f4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1800533fabfa4879b79ab7c6eca67835":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4a483abdcfa84628a7f57a4af4aa96b0","placeholder":"​","style":"IPY_MODEL_e4e65ec693ee4c1a899477b0d43289e5","value":"100%"}},"1c3bf5f50434405584c19110d5435dbc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1f2f009e9123422eac636b7185e5e5bc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1f50a03ed1fb4cdba8de197e8c1baf53":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0e73f5fd8a3c452db544b32f31d92b58","max":468,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6cfa6efa911e4db394ac28ce4a5e30bb","value":468}},"239dda124426456b85694f296c97f43e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a837b4ed77934a0fbc1635b7fb355878","IPY_MODEL_714f7aa59df64b7c85f56b171dc21819","IPY_MODEL_6290747c4b86403c9c9da7700484e08b"],"layout":"IPY_MODEL_9f3bd4cf01524670a917a2ac06028c88"}},"2452782a14a8467b81c3ec1952eab57a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2930b5d093ef49e78619d6cf4306346c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2ab5911f14b7470b9d84e2a8f6c61742":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2b67862cb2a64303be5b19104d486f1e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b97a66f249249f7b6678b9e623d347a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0244679fb5e945aab3d1438f135f604c","IPY_MODEL_f57f45a822ea4a549a0900976cef626d","IPY_MODEL_b932a2f2bd9948efabb9963ce1ec507d"],"layout":"IPY_MODEL_4176c7a00f604d958aab0bc1b9a18d61"}},"2bb80c2b9cbc45b4a15cab8b49d4a5ca":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2c14a2b9217e4bbabc13cf21bb994836":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2c56ccb755474dd5aa4da67cbb03eb53":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9a49604021544772a4ae602e7c80fa73","placeholder":"​","style":"IPY_MODEL_38eeb63deb60493298c50a2ebfa94127","value":"100%"}},"2e379acac22b4ad4a93c3a52f4d2be75":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1800533fabfa4879b79ab7c6eca67835","IPY_MODEL_65c429d251594e5e8aad585b8efeb74c","IPY_MODEL_077c4023a39b4c74b83b7f04f7c24b4a"],"layout":"IPY_MODEL_bee9d72b6b0b46499b804db512f8bb0c"}},"2f711a498b2d4fcbb607bc39d505b9bb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"38eeb63deb60493298c50a2ebfa94127":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3940d6e1237f4c36b7fd0daa00881ace":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4176c7a00f604d958aab0bc1b9a18d61":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"41d10014575f4d8bb2a3b9e87e5d1185":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_48a393bef3ef48169dcdf08348c26008","placeholder":"​","style":"IPY_MODEL_94205b81f2734a7fa723c17c3039c69c","value":"100%"}},"45c784d3cc5c4da19607f87f434d6639":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2b67862cb2a64303be5b19104d486f1e","placeholder":"​","style":"IPY_MODEL_a437f6ed0aa842c29ca4dcc6746b0f0e","value":"100%"}},"4682a42aba574b0ab3152967ad1e93d5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_899ff269a937476c85e65a6047c44902","max":468,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2930b5d093ef49e78619d6cf4306346c","value":36}},"4881cad3472a4ea7b51d1a6056886cc3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"48a393bef3ef48169dcdf08348c26008":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"48b2ca1b8ca843c0aca5d8035e47d853":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ffac8ebbb66646979a6e15aa58f0ef37","IPY_MODEL_4682a42aba574b0ab3152967ad1e93d5","IPY_MODEL_4a091893e2794b94b9e394c9505dfd66"],"layout":"IPY_MODEL_7bb64d0fddba499b88df113d55204f47"}},"4a091893e2794b94b9e394c9505dfd66":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cdd22aa57df8492a91e5a7c836fcac40","placeholder":"​","style":"IPY_MODEL_2bb80c2b9cbc45b4a15cab8b49d4a5ca","value":" 36/468 [00:20&lt;04:01,  1.79it/s]"}},"4a483abdcfa84628a7f57a4af4aa96b0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5629719253f84b1a8577927b54d5f62a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e092174e8e264773871a898d9fc4d25b","placeholder":"​","style":"IPY_MODEL_7f2954833f9844c195d9e1255b50b482","value":"100%"}},"5a8d980c9dc949168eb9001601792d80":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d2291ca69fa04963841b42513a70e200","IPY_MODEL_73b50b70009a4c8aad0b267169eb240d","IPY_MODEL_e2d8c453a1084cd4a625eee988ca46ba"],"layout":"IPY_MODEL_7470c09d93404cfd9e8b01251b0e973d"}},"5d1efffaef7a4c008a58fccb2db521dc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6dd5c10800b645c2ad2d9b4179190102","placeholder":"​","style":"IPY_MODEL_aa74b06872484e169992570798568ebf","value":" 468/468 [04:26&lt;00:00,  1.76it/s]"}},"61c6328f4f0a456a978593bf93bf5374":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"624d74fa30b24b0dabfa16e44894282a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6290747c4b86403c9c9da7700484e08b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7238023739024bc280fb4cf7fa38e51c","placeholder":"​","style":"IPY_MODEL_17990007f8ac472a8794685bba7f67f4","value":" 468/468 [04:30&lt;00:00,  1.73it/s]"}},"65c429d251594e5e8aad585b8efeb74c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2c14a2b9217e4bbabc13cf21bb994836","max":468,"min":0,"orientation":"horizontal","style":"IPY_MODEL_763a069d1b6049d983d63cedf1f1e4b8","value":468}},"6c06e9cdbfa344bba7603bd41c178ed1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6cfa6efa911e4db394ac28ce4a5e30bb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6dd5c10800b645c2ad2d9b4179190102":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"714f7aa59df64b7c85f56b171dc21819":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ca6a3d45538c4ecc9510f73bf66b8287","max":468,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3940d6e1237f4c36b7fd0daa00881ace","value":468}},"7238023739024bc280fb4cf7fa38e51c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73b50b70009a4c8aad0b267169eb240d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e638e2177232429ba63c93e6cdb873a2","max":468,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ddca176f9058475493fb5a0d20f9511b","value":468}},"7470c09d93404cfd9e8b01251b0e973d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"763a069d1b6049d983d63cedf1f1e4b8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"779538be6b4d4283974efeee02e662a4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7bb64d0fddba499b88df113d55204f47":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f01b352942442919f3416b21fab5b8e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_94cc2ac4c3f34b9ca95e9fab261b8502","placeholder":"​","style":"IPY_MODEL_624d74fa30b24b0dabfa16e44894282a","value":" 468/468 [04:23&lt;00:00,  1.80it/s]"}},"7f2954833f9844c195d9e1255b50b482":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8016868940b2406a90793aedf3ebd091":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"81909bce33d04fc4bb63164cfe17fea6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8799efe939a54cce829696532ffcc737":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"899ff269a937476c85e65a6047c44902":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a3b268c41c54dbabdf8378e3c775114":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"904b2f38a95e4deba95836b4a8c84c85":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"94205b81f2734a7fa723c17c3039c69c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"94cc2ac4c3f34b9ca95e9fab261b8502":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"994ef49b00304689877b3f3c0a69e397":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0d6ec13ee7124458ae7c1b627aba69a4","max":468,"min":0,"orientation":"horizontal","style":"IPY_MODEL_151c396c9619420d9ac72220d1b08b47","value":468}},"9a49604021544772a4ae602e7c80fa73":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9f0e3e2991674b9c9e2c5eecd5521cc2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0ff8397601594b68b0d17ba35a4f37d5","placeholder":"​","style":"IPY_MODEL_e6f2045ec53a4c319090ad6832922e0b","value":" 468/468 [04:25&lt;00:00,  1.72it/s]"}},"9f3bd4cf01524670a917a2ac06028c88":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a1fdea7d6f9c4f88beca5f007b4361d2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a2f44441f69249d8a15ad86e771bcfe5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a437f6ed0aa842c29ca4dcc6746b0f0e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a837b4ed77934a0fbc1635b7fb355878":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_df97998b31ba47ae993e8a1360398237","placeholder":"​","style":"IPY_MODEL_051afe5474c44b5c868c537f7bff80a4","value":"100%"}},"a8b028d16c1c4c28b35e62e57ad41411":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c6ec17e84cdb427496154a5aebfc8f09","max":468,"min":0,"orientation":"horizontal","style":"IPY_MODEL_abf594b1d4494c13886c63ffb709377c","value":468}},"aa74b06872484e169992570798568ebf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aac6f7b4a2cf4bb494384d24f4a7c192":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"abf594b1d4494c13886c63ffb709377c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b932a2f2bd9948efabb9963ce1ec507d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aac6f7b4a2cf4bb494384d24f4a7c192","placeholder":"​","style":"IPY_MODEL_81909bce33d04fc4bb63164cfe17fea6","value":" 468/468 [04:28&lt;00:00,  1.76it/s]"}},"bca4cd03641d4084a095f2fa161be586":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6c06e9cdbfa344bba7603bd41c178ed1","placeholder":"​","style":"IPY_MODEL_8a3b268c41c54dbabdf8378e3c775114","value":" 468/468 [04:23&lt;00:00,  1.79it/s]"}},"bee9d72b6b0b46499b804db512f8bb0c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6ec17e84cdb427496154a5aebfc8f09":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c7744805ad6c4e1d8c72a7fb0f613239":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2c56ccb755474dd5aa4da67cbb03eb53","IPY_MODEL_1f50a03ed1fb4cdba8de197e8c1baf53","IPY_MODEL_5d1efffaef7a4c008a58fccb2db521dc"],"layout":"IPY_MODEL_1562625b6bab4479885677018a8ddfe9"}},"ca6a3d45538c4ecc9510f73bf66b8287":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cdd22aa57df8492a91e5a7c836fcac40":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d08391037342464f8332f086f4efb57e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d2291ca69fa04963841b42513a70e200":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d08391037342464f8332f086f4efb57e","placeholder":"​","style":"IPY_MODEL_ec86a55df49f4526b348b3f2154ecfe9","value":"100%"}},"d3e71dd9684249c5a22ee42efe60171b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc4729a82d2c430a839d224a7574ea52":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc95e7727196462b96ba30bc4791f953":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5629719253f84b1a8577927b54d5f62a","IPY_MODEL_a8b028d16c1c4c28b35e62e57ad41411","IPY_MODEL_9f0e3e2991674b9c9e2c5eecd5521cc2"],"layout":"IPY_MODEL_4881cad3472a4ea7b51d1a6056886cc3"}},"ddca176f9058475493fb5a0d20f9511b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"df97998b31ba47ae993e8a1360398237":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e092174e8e264773871a898d9fc4d25b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e1d81d1ec0d14e25a32bba2544746f31":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e26b01b40de041798d84f4d474ca2a49":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e2d8c453a1084cd4a625eee988ca46ba":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2f711a498b2d4fcbb607bc39d505b9bb","placeholder":"​","style":"IPY_MODEL_904b2f38a95e4deba95836b4a8c84c85","value":" 468/468 [04:21&lt;00:00,  1.78it/s]"}},"e2ecca5322da4b69a3a0640837a429e5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e4e65ec693ee4c1a899477b0d43289e5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e638e2177232429ba63c93e6cdb873a2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e6f2045ec53a4c319090ad6832922e0b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e844c1cb1c114ec48d5b4b2c41b2a1e7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_45c784d3cc5c4da19607f87f434d6639","IPY_MODEL_fc00d61c091c4a05bf1d81a3c1ea04b4","IPY_MODEL_bca4cd03641d4084a095f2fa161be586"],"layout":"IPY_MODEL_13d68b2cf7d746089f3950f15415c0bf"}},"e9ae609663864f1bb0822954ab462f40":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d3e71dd9684249c5a22ee42efe60171b","placeholder":"​","style":"IPY_MODEL_a1fdea7d6f9c4f88beca5f007b4361d2","value":" 468/468 [04:25&lt;00:00,  1.77it/s]"}},"ec86a55df49f4526b348b3f2154ecfe9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f315cc71d0404c0e8b192d6c536cb457":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f516c8753a4e4825a3dfe4aeeb41df69":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8016868940b2406a90793aedf3ebd091","placeholder":"​","style":"IPY_MODEL_1590dc87d71945b2a76f8752931562de","value":"100%"}},"f57f45a822ea4a549a0900976cef626d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e2ecca5322da4b69a3a0640837a429e5","max":468,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1f2f009e9123422eac636b7185e5e5bc","value":468}},"fc00d61c091c4a05bf1d81a3c1ea04b4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0b91e193018c474e9746eab260e4dc69","max":468,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a2f44441f69249d8a15ad86e771bcfe5","value":468}},"ffac8ebbb66646979a6e15aa58f0ef37":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc4729a82d2c430a839d224a7574ea52","placeholder":"​","style":"IPY_MODEL_779538be6b4d4283974efeee02e662a4","value":"  8%"}},"7ee7fc9855c84dd38e7763621fe976d6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_90642b9f04784e8caa493274e7168c86","IPY_MODEL_a00c82f11f3f4cb7857cc7bba00e760f","IPY_MODEL_616e503b11674b53adf417798c574b51"],"layout":"IPY_MODEL_b624274833b946a58410435cf3d90176"}},"90642b9f04784e8caa493274e7168c86":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_328083add2cb4e47a76cd82de0309684","placeholder":"​","style":"IPY_MODEL_beebe7ff6aff477ca331016019ce7206","value":"100%"}},"a00c82f11f3f4cb7857cc7bba00e760f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4833a2d094ef4eeeaf18b11e88e442bc","max":468,"min":0,"orientation":"horizontal","style":"IPY_MODEL_422f9f9c11b14b5e9a181d6e5d92427f","value":468}},"616e503b11674b53adf417798c574b51":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_af218bc723704cedbfa72303e839c7e3","placeholder":"​","style":"IPY_MODEL_3cfaaad927414ea2a5351a48b83e2ef7","value":" 468/468 [04:43&lt;00:00,  1.52it/s]"}},"b624274833b946a58410435cf3d90176":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"328083add2cb4e47a76cd82de0309684":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"beebe7ff6aff477ca331016019ce7206":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4833a2d094ef4eeeaf18b11e88e442bc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"422f9f9c11b14b5e9a181d6e5d92427f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"af218bc723704cedbfa72303e839c7e3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3cfaaad927414ea2a5351a48b83e2ef7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"463d43f7a14c4b79a1446b2219c3b9b3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_549a46686beb4f87a12107b08204f914","IPY_MODEL_a160db07ab1e409eafb63d0136ebb5e8","IPY_MODEL_f984dabaa4574ef290157fcf5027fee6"],"layout":"IPY_MODEL_b3e0cd7042234abab721e227104f952e"}},"549a46686beb4f87a12107b08204f914":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_69c64a8c288f464c9b9e33b63b3607b8","placeholder":"​","style":"IPY_MODEL_2384e30d037e429aa8390b0778c40818","value":"100%"}},"a160db07ab1e409eafb63d0136ebb5e8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d150c8d90e6840bb8c7456dad8330c33","max":468,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cec11e5768c2412e82c96e7f4fb5470a","value":468}},"f984dabaa4574ef290157fcf5027fee6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a84e395b605f4438b31ee1b8b72ea2cc","placeholder":"​","style":"IPY_MODEL_cf6bc2d4316340a29242f18ff704f9b2","value":" 468/468 [05:24&lt;00:00,  1.33it/s]"}},"b3e0cd7042234abab721e227104f952e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"69c64a8c288f464c9b9e33b63b3607b8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2384e30d037e429aa8390b0778c40818":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d150c8d90e6840bb8c7456dad8330c33":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cec11e5768c2412e82c96e7f4fb5470a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a84e395b605f4438b31ee1b8b72ea2cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cf6bc2d4316340a29242f18ff704f9b2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cfa3cf9832664ab68d635af2e3e70abd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1b649ffe17734c95bcfbfde5299ac9b1","IPY_MODEL_bd50cf46137d416db7567858b7d4d457","IPY_MODEL_0edfc1e141754cfc8e9b4948133d9167"],"layout":"IPY_MODEL_a2e3e2b9b5804dcb8c3972f90c02e605"}},"1b649ffe17734c95bcfbfde5299ac9b1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1863d161dd5b42f897f67c5488391bd6","placeholder":"​","style":"IPY_MODEL_b1c2ad2ff892415e99406a442e245278","value":"100%"}},"bd50cf46137d416db7567858b7d4d457":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_de83661990f9432b9fccf33f78d4a17b","max":468,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3fa2ad5fbb794b439648cab271c89fa2","value":468}},"0edfc1e141754cfc8e9b4948133d9167":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7137e8d10ab94740859e62be55b41043","placeholder":"​","style":"IPY_MODEL_d619d6ab2cbe4d4680d6746bac9c1c32","value":" 468/468 [06:03&lt;00:00,  1.26it/s]"}},"a2e3e2b9b5804dcb8c3972f90c02e605":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1863d161dd5b42f897f67c5488391bd6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b1c2ad2ff892415e99406a442e245278":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"de83661990f9432b9fccf33f78d4a17b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3fa2ad5fbb794b439648cab271c89fa2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7137e8d10ab94740859e62be55b41043":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d619d6ab2cbe4d4680d6746bac9c1c32":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7016df46c00c406f961339721fe3c717":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_49f449a8930a4a7199ddd3711fb404c7","IPY_MODEL_a38cb53c338d42798e291dda2d010238","IPY_MODEL_918f3cef52904a97a4061c59b223709a"],"layout":"IPY_MODEL_d26cdf3cccaa4757a40804cf0365cf7f"}},"49f449a8930a4a7199ddd3711fb404c7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_911f5d82a0e44d9e95d32b3aa7a911c0","placeholder":"​","style":"IPY_MODEL_5e3b933856c3464ca7ea10b7332bfb94","value":"100%"}},"a38cb53c338d42798e291dda2d010238":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_849882c879334116a2d03d26b89845ee","max":468,"min":0,"orientation":"horizontal","style":"IPY_MODEL_85b0ad2bf114459d8454032abce5d190","value":468}},"918f3cef52904a97a4061c59b223709a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bef333d303b542b0abae977620f4af67","placeholder":"​","style":"IPY_MODEL_eb5a177384a3498b84c6c7290283ca7f","value":" 468/468 [06:12&lt;00:00,  1.26it/s]"}},"d26cdf3cccaa4757a40804cf0365cf7f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"911f5d82a0e44d9e95d32b3aa7a911c0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e3b933856c3464ca7ea10b7332bfb94":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"849882c879334116a2d03d26b89845ee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"85b0ad2bf114459d8454032abce5d190":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bef333d303b542b0abae977620f4af67":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb5a177384a3498b84c6c7290283ca7f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f161f32f76194d5aa7e6ee87d818e3d6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e69c0c458e8f432cbae236d19a29f085","IPY_MODEL_ae44d8c8fead4673abc63ed13dc80459","IPY_MODEL_c12d9206a37c43b3bfdd78f44357eb08"],"layout":"IPY_MODEL_1430cc1101064cc6acca731270a3ec7e"}},"e69c0c458e8f432cbae236d19a29f085":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fe4693c8fc344288bca1d1c9be65a915","placeholder":"​","style":"IPY_MODEL_7b1c77fa858b4a41a78092e223c32566","value":"100%"}},"ae44d8c8fead4673abc63ed13dc80459":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f4e0819d6dac42489d2146183bc6ed88","max":468,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9f7013a518cb4865a8f03e16e60813b6","value":468}},"c12d9206a37c43b3bfdd78f44357eb08":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_10399975f60b4df78dab905b15909b24","placeholder":"​","style":"IPY_MODEL_a69fa222ab894644af450c43654f7c71","value":" 468/468 [06:19&lt;00:00,  1.22it/s]"}},"1430cc1101064cc6acca731270a3ec7e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fe4693c8fc344288bca1d1c9be65a915":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b1c77fa858b4a41a78092e223c32566":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f4e0819d6dac42489d2146183bc6ed88":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9f7013a518cb4865a8f03e16e60813b6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"10399975f60b4df78dab905b15909b24":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a69fa222ab894644af450c43654f7c71":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6deef8d9e58547ae85034c08d257c5bc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a7108a0a89c941ef8559588e8150b908","IPY_MODEL_8dda673f6df249c59998d71931f3d34a","IPY_MODEL_a6d7fbc3e51946e59b948f900e50bb37"],"layout":"IPY_MODEL_059964edea5248f7be197cd180b2e706"}},"a7108a0a89c941ef8559588e8150b908":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_18a8df6c699c45b3af3239970a57093d","placeholder":"​","style":"IPY_MODEL_6d5833e0dcdc4c4d9cd42eac8e2c1c57","value":"100%"}},"8dda673f6df249c59998d71931f3d34a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6431372503db4eee982ea2defd8df27c","max":468,"min":0,"orientation":"horizontal","style":"IPY_MODEL_02316d57d4e94ca395540d35b7d8d9b0","value":468}},"a6d7fbc3e51946e59b948f900e50bb37":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_baaee8df989845c6a4cb588d82741061","placeholder":"​","style":"IPY_MODEL_d6dab98868c74a7289f37f16fab408ed","value":" 468/468 [06:04&lt;00:00,  1.49it/s]"}},"059964edea5248f7be197cd180b2e706":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"18a8df6c699c45b3af3239970a57093d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6d5833e0dcdc4c4d9cd42eac8e2c1c57":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6431372503db4eee982ea2defd8df27c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"02316d57d4e94ca395540d35b7d8d9b0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"baaee8df989845c6a4cb588d82741061":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d6dab98868c74a7289f37f16fab408ed":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"88c21610bc084d1ca2f538d5d812c806":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1682d8b2a4964335808f4fc83cf064cc","IPY_MODEL_7572dc7bb77a4d5fa7d5d1c2cac4ee4a","IPY_MODEL_7511a0995867456191a5d8eb688d778b"],"layout":"IPY_MODEL_0afe963dc1284c60a1c18becb042a87a"}},"1682d8b2a4964335808f4fc83cf064cc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8d92a1f7e4134173a3488714b2924ace","placeholder":"​","style":"IPY_MODEL_d527b5d90cee42698d8dd4e5f121f144","value":"100%"}},"7572dc7bb77a4d5fa7d5d1c2cac4ee4a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ca5b7b901c83463db2979dd406db3f67","max":468,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3f3326f8bca940fba05249f65aea88c8","value":468}},"7511a0995867456191a5d8eb688d778b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_583ef6d228fa4f97b6173641bd24148e","placeholder":"​","style":"IPY_MODEL_7b34b323475b451fb2857a952d7598e0","value":" 468/468 [05:00&lt;00:00,  1.59it/s]"}},"0afe963dc1284c60a1c18becb042a87a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d92a1f7e4134173a3488714b2924ace":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d527b5d90cee42698d8dd4e5f121f144":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ca5b7b901c83463db2979dd406db3f67":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f3326f8bca940fba05249f65aea88c8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"583ef6d228fa4f97b6173641bd24148e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b34b323475b451fb2857a952d7598e0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2204855fb0e04a229fa435767922fa77":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7a28e244546d485c921682d87357e5cc","IPY_MODEL_617dcbc454224305b624214a40b0c5eb","IPY_MODEL_fd706e5c2ae04c558c8eecdfcd2f7a95"],"layout":"IPY_MODEL_c410ff08a43743f0b7bf0c75e618e710"}},"7a28e244546d485c921682d87357e5cc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e10b77fab9e64b3fbd9372964f851b26","placeholder":"​","style":"IPY_MODEL_2919fafaa4a7439a9ac74f5ac32cf7a9","value":"100%"}},"617dcbc454224305b624214a40b0c5eb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_86d790d84c8d4654868219add9e07e50","max":468,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c4423fd84d7640f19333bd764c5e4ea8","value":468}},"fd706e5c2ae04c558c8eecdfcd2f7a95":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a0efd20f3396424e96d58d61e59c7442","placeholder":"​","style":"IPY_MODEL_51b49435652b4442bb75ad64088e16eb","value":" 468/468 [04:54&lt;00:00,  1.59it/s]"}},"c410ff08a43743f0b7bf0c75e618e710":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e10b77fab9e64b3fbd9372964f851b26":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2919fafaa4a7439a9ac74f5ac32cf7a9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"86d790d84c8d4654868219add9e07e50":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c4423fd84d7640f19333bd764c5e4ea8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a0efd20f3396424e96d58d61e59c7442":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"51b49435652b4442bb75ad64088e16eb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"104e84ce46fa4abf97e0e8588b5c5ffa":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b1b20181b0364ed09c01a5a4616cdeb9","IPY_MODEL_c8aff08af2014f00b53849f9b0be3d1d","IPY_MODEL_6a7f605bdd2347ecaf5a79510c1f738f"],"layout":"IPY_MODEL_ae8a495d8759472db88281f529000983"}},"b1b20181b0364ed09c01a5a4616cdeb9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e0848a9dd81b4d8d8204b2ac873b54b6","placeholder":"​","style":"IPY_MODEL_6724b4953ca14088b60adb41608b7a7f","value":"100%"}},"c8aff08af2014f00b53849f9b0be3d1d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fdbcccc24872405192bd7d6578941fdb","max":468,"min":0,"orientation":"horizontal","style":"IPY_MODEL_00da45ab29da4097929150cd3a3a18e9","value":468}},"6a7f605bdd2347ecaf5a79510c1f738f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7c258f8e269b471db37e58cfa28b2056","placeholder":"​","style":"IPY_MODEL_a3b7b32f6e9e40ab8b00e87641a29081","value":" 468/468 [04:55&lt;00:00,  1.60it/s]"}},"ae8a495d8759472db88281f529000983":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e0848a9dd81b4d8d8204b2ac873b54b6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6724b4953ca14088b60adb41608b7a7f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fdbcccc24872405192bd7d6578941fdb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"00da45ab29da4097929150cd3a3a18e9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7c258f8e269b471db37e58cfa28b2056":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a3b7b32f6e9e40ab8b00e87641a29081":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9b2c7035270540cfbf1878c5b2c8ff6a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d7ff01a223f843c3ad73aeb2f22258c2","IPY_MODEL_f1d8af5ed5d249e985e3d1da9dfed334","IPY_MODEL_aa502697e5dd4ada9faf2b2e459d3831"],"layout":"IPY_MODEL_e0097b4f83bf484bb8c43a573a785bd8"}},"d7ff01a223f843c3ad73aeb2f22258c2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc113608c7164d0a996c23c4c83a5502","placeholder":"​","style":"IPY_MODEL_4670c20c63664504be259ce007f40d4d","value":"100%"}},"f1d8af5ed5d249e985e3d1da9dfed334":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a2cd6b784ac942b88a51a31bea8353b4","max":468,"min":0,"orientation":"horizontal","style":"IPY_MODEL_049d64d493904c578448beef35bf02b8","value":468}},"aa502697e5dd4ada9faf2b2e459d3831":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a31e21b6904c45c7916210a3a82b5fe5","placeholder":"​","style":"IPY_MODEL_8f2532f1c4e941409c1d3023a52acbab","value":" 468/468 [04:57&lt;00:00,  1.60it/s]"}},"e0097b4f83bf484bb8c43a573a785bd8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc113608c7164d0a996c23c4c83a5502":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4670c20c63664504be259ce007f40d4d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a2cd6b784ac942b88a51a31bea8353b4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"049d64d493904c578448beef35bf02b8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a31e21b6904c45c7916210a3a82b5fe5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8f2532f1c4e941409c1d3023a52acbab":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}